{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4bacc318390456b",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook focuses on finetuning the Prithvi EO v2.0 model to classify crops in a HLS scene. The main take aways from this notebook will be as follows:\n",
    "1. Learn how to use Terratorch to finetune Prithvi EO v2.0 300m for crop classification (13 classes).\n",
    "2. Use Huggingface datasets with Prithvi EO.\n",
    "3. Understand the effects of spefic parameters in training and hardware utilization\n",
    "4. Use finetuned model for inference.\n",
    "\n",
    "## Setup\n",
    "1. Go to \"Kernel\"\n",
    "2. Select \"prithvi_eo\""
   ]
  },
  {
   "cell_type": "code",
   "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a",
   "metadata": {
    "id": "2e8c1961-311b-49e0-b6ea-2867b44cb47a"
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import gdown\n",
    "import terratorch\n",
    "import albumentations\n",
    "import lightning.pytorch as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from terratorch.datamodules import MultiTemporalCropClassificationDataModule\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "917b65b8e7cd7d65",
   "metadata": {},
   "source": [
    "3. Download the dataset from Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "id": "3t-YKKUztjXn",
   "metadata": {
    "id": "3t-YKKUztjXn",
    "scrolled": true
   },
   "source": [
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "dataset_path = \"../data/multi-temporal-crop-classification\"\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"ibm-nasa-geospatial/multi-temporal-crop-classification\",\n",
    "    allow_patterns=\"*.tgz\",\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=dataset_path,\n",
    ")\n",
    "snapshot_download(\n",
    "    repo_id=\"ibm-nasa-geospatial/multi-temporal-crop-classification\",\n",
    "    allow_patterns=\"*.txt\",\n",
    "    repo_type=\"dataset\",\n",
    "    local_dir=dataset_path,\n",
    ")\n",
    "!mkdir ../data/multi-temporal-crop-classification/training_chips; tar -xzf ../data/multi-temporal-crop-classification/training_chips.tgz -C ../data/multi-temporal-crop-classification/\n",
    "!mkdir ../data/multi-temporal-crop-classification/validation_chips; tar -xzf ../data/multi-temporal-crop-classification/validation_chips.tgz -C ../data/multi-temporal-crop-classification/\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "35ba4d58-8ff6-4f9c-bfb1-a70376f80494",
   "metadata": {
    "id": "35ba4d58-8ff6-4f9c-bfb1-a70376f80494"
   },
   "source": [
    "## Multi-temporal Crop Dataset\n",
    "\n",
    "Lets start with analyzing the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e3854bdb-17a4-43c8-bfa8-822b44fd59c3",
   "metadata": {
    "id": "e3854bdb-17a4-43c8-bfa8-822b44fd59c3"
   },
   "source": [
    "!ls \"{dataset_path}\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ddd7d83440895e87",
   "metadata": {},
   "source": [
    "# Each merged sample includes the stacked bands of three time steps\n",
    "!ls \"{dataset_path}/training_chips\" | head"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ecd1487c",
   "metadata": {},
   "source": [
    "# Parameters to modify\n",
    "batch_size = 4\n",
    "num_workers = 2\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "prithvi_backbone = \"prithvi_eo_v2_300_tl\", # Model can be either prithvi_eo_v1_100, prithvi_eo_v2_300, prithvi_eo_v2_300_tl, prithvi_eo_v2_600, prithvi_eo_v2_600_tl\n",
    "\n",
    "\n",
    "indices = [5, 11, 17, 23]\n",
    "\n",
    "if prithvi_backbone == 'prithvi_eo_v2_100':\n",
    "    indices = [2, 5, 8, 11] # indices for prithvi_eo_v1_100\n",
    "elif prithvi_backbone == 'prithvi_eo_v2_300' or prithvi_backbone == 'prithvi_eo_v2_300_tl': \n",
    "    indices = [5, 11, 17, 23] # indices for prithvi_eo_v2_300\n",
    "elif prithvi_backbone == 'prithvi_eo_v2_600' or prithvi_backbone == 'prithvi_eo_v2_600_tl':\n",
    "    indices = [7, 15, 23, 31] # indices for prithvi_eo_v2_600\n",
    "\n",
    "# Total number of epochs the training will run for. Since we are short on time, we will just be running it for 1 epoch. This can be updated to any positive integer.\n",
    "max_epochs = 1 \n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "735803b1-a4bf-427f-a1e6-5ac755af33fc",
   "metadata": {
    "id": "735803b1-a4bf-427f-a1e6-5ac755af33fc"
   },
   "source": [
    "# Adjusted dataset class for this dataset (general dataset could be used as well)\n",
    "datamodule = MultiTemporalCropClassificationDataModule(\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    data_root=dataset_path,\n",
    "    train_transform=[\n",
    "        terratorch.datasets.transforms.FlattenTemporalIntoChannels(),  # Required for temporal data\n",
    "        albumentations.D4(), # Random flips and rotation\n",
    "        albumentations.pytorch.transforms.ToTensorV2(),\n",
    "        terratorch.datasets.transforms.UnflattenTemporalFromChannels(n_timesteps=3),\n",
    "    ],\n",
    "    val_transform=None,  # Using ToTensor() by default\n",
    "    test_transform=None,\n",
    "    expand_temporal_dimension=True,\n",
    "    use_metadata=False, # The crop dataset has metadata for location and time\n",
    "    reduce_zero_label=True,\n",
    ")\n",
    "\n",
    "# Setup train and val datasets\n",
    "datamodule.setup(\"fit\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a87ed3b7-f7dc-486d-ac59-cd781a070925",
   "metadata": {
    "id": "a87ed3b7-f7dc-486d-ac59-cd781a070925"
   },
   "source": [
    "# Mean and standard deviation calculated from the training dataset for all 6 bands, and 3 timesteps, for zero mean normalization.\n",
    "# checking for the dataset means and stds\n",
    "datamodule.means, datamodule.stds"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "08644e71-d82f-426c-b0c1-79026fccb578",
   "metadata": {
    "id": "08644e71-d82f-426c-b0c1-79026fccb578"
   },
   "source": [
    "# checking datasets train split size\n",
    "train_dataset = datamodule.train_dataset\n",
    "len(train_dataset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "88b86821-3481-4d92-bdba-246568c66c48",
   "metadata": {
    "id": "88b86821-3481-4d92-bdba-246568c66c48"
   },
   "source": [
    "# checking datasets available bands\n",
    "train_dataset.all_band_names"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9264de41-ab16-43cc-9ea2-ee51b0969624",
   "metadata": {
    "id": "9264de41-ab16-43cc-9ea2-ee51b0969624"
   },
   "source": [
    "# checking datasets classes\n",
    "train_dataset.class_names"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3a1da2ad-a797-4f4a-ad1a-cd10f9addb01",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3a1da2ad-a797-4f4a-ad1a-cd10f9addb01",
    "outputId": "9c948b7c-e02b-4980-a142-b36bcb51a8e4"
   },
   "source": [
    "# plotting a few samples\n",
    "for i in range(5):\n",
    "    train_dataset.plot(train_dataset[i])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b7062ddc-a3b7-4378-898c-41abcdf2ee3b",
   "metadata": {
    "id": "b7062ddc-a3b7-4378-898c-41abcdf2ee3b"
   },
   "source": [
    "# checking datasets validation split size\n",
    "val_dataset = datamodule.val_dataset\n",
    "len(val_dataset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ede1c1c6-9f60-4510-a2da-572c55d03f79",
   "metadata": {
    "id": "ede1c1c6-9f60-4510-a2da-572c55d03f79"
   },
   "source": [
    "# checking datasets testing split size\n",
    "datamodule.setup(\"test\")\n",
    "test_dataset = datamodule.test_dataset\n",
    "len(test_dataset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4072e2f849c0df2d",
   "metadata": {},
   "source": [
    "# Fine-tune Prithvi"
   ]
  },
  {
   "cell_type": "code",
   "id": "ae69d39a-857a-4392-b058-0f4b518edf6e",
   "metadata": {
    "id": "ae69d39a-857a-4392-b058-0f4b518edf6e",
    "scrolled": true
   },
   "source": [
    "pl.seed_everything(0)\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=\"../output/multicrop/checkpoints/\",\n",
    "    mode=\"max\",\n",
    "    monitor=\"val/Multiclass_Jaccard_Index\", # Variable to monitor\n",
    "    filename=\"best-{epoch:02d}\",\n",
    ")\n",
    "\n",
    "# Lightning Trainer\n",
    "trainer = pl.Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    strategy=\"auto\",\n",
    "    devices=1, # Lightning multi-gpu often fails in notebooks\n",
    "    precision='bf16-mixed',  # Speed up training\n",
    "    num_nodes=1,\n",
    "    logger=True, # Uses TensorBoard by default\n",
    "    max_epochs=1, # For demos\n",
    "    log_every_n_steps=5,\n",
    "    enable_checkpointing=True,\n",
    "    callbacks=[checkpoint_callback, pl.callbacks.RichProgressBar()],\n",
    "    default_root_dir=\"../output/multicrop\",\n",
    ")\n",
    "\n",
    "# Model\n",
    "model = terratorch.tasks.SemanticSegmentationTask(\n",
    "    model_factory=\"EncoderDecoderFactory\",\n",
    "    model_args={\n",
    "        # Backbone\n",
    "        \"backbone\": \"prithvi_eo_v2_300_tl\",\n",
    "        \"backbone_pretrained\": True,\n",
    "        \"backbone_num_frames\": 3,\n",
    "        \"backbone_bands\": [\"BLUE\", \"GREEN\", \"RED\", \"NIR_NARROW\", \"SWIR_1\", \"SWIR_2\"],\n",
    "        \"backbone_coords_encoding\": [], # use [\"time\", \"location\"] for time and location metadata\n",
    "        \n",
    "        # Necks \n",
    "        \"necks\": [\n",
    "            {\n",
    "                \"name\": \"SelectIndices\",\n",
    "                # \"indices\": [2, 5, 8, 11]  # 100m model\n",
    "                \"indices\": [5, 11, 17, 23]  # 300m model\n",
    "                # \"indices\": [7, 15, 23, 31]  # 300m model\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"ReshapeTokensToImage\",\n",
    "                \"effective_time_dim\": 3\n",
    "            },\n",
    "            {\"name\": \"LearnedInterpolateToPyramidal\"},\n",
    "        ],\n",
    "        \n",
    "        # Decoder\n",
    "        \"decoder\": \"UNetDecoder\",\n",
    "        \"decoder_channels\": [512, 256, 128, 64],\n",
    "        \n",
    "        # Head\n",
    "        \"head_dropout\": 0.1,\n",
    "        \"num_classes\": 13,\n",
    "    },\n",
    "\n",
    "    loss=\"ce\",\n",
    "    lr=1e-4,\n",
    "    optimizer=\"AdamW\",\n",
    "    ignore_index=-1,\n",
    "    freeze_backbone=True,  # Speeds up fine-tuning\n",
    "    freeze_decoder=False,\n",
    "    plot_on_val=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# You can start the tensorboard with (run it in a terminal window)\n",
    "!pip install tensorboard\n",
    "!tensorboard --logdir output &\n",
    "\n",
    "# tensorboard can be accessed by updating the `lab` part in the current jupyterlab browser tab with `/proxy/6006/`:\n",
    "# Eg: https://gvipa9zcdsccwe6.studio.us-west-2.sagemaker.aws/jupyterlab/default/lab -> https://gvipa9zcdsccwe6.studio.us-west-2.sagemaker.aws/jupyterlab/default/proxy/6006/"
   ],
   "id": "9a3ce82c94494fa1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Training\n",
    "trainer.fit(model, datamodule=datamodule)"
   ],
   "id": "27fee1e72be7349"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test the fine-tuned model",
   "id": "2d9c27a7aa4f21ea"
  },
  {
   "cell_type": "code",
   "id": "388aa3db0dc07460",
   "metadata": {},
   "source": [
    "best_ckpt_path = \"../output/multicrop/checkpoints/best-epoch=00.ckpt\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "59019da2-138c-4cd6-95f0-72bb6d6f1dec",
   "metadata": {},
   "source": [
    "def run_test_and_plot(model, ckpt_path):\n",
    "\n",
    "    # calculate test metrics\n",
    "    trainer.test(model, datamodule=datamodule, ckpt_path=ckpt_path)\n",
    "\n",
    "    # get predictions\n",
    "    preds = trainer.predict(model, datamodule=datamodule, ckpt_path=ckpt_path)\n",
    "\n",
    "    # get data \n",
    "    data_loader = trainer.predict_dataloaders\n",
    "    batch = next(iter(data_loader))\n",
    "\n",
    "    # plot\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        sample = {key: batch[key][i] for key in batch}\n",
    "        sample[\"prediction\"] = preds[0][0][0][i].cpu().numpy()\n",
    "\n",
    "        datamodule.predict_dataset.plot(sample)\n",
    "        "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aa6c8add-d53d-4e1e-a6f9-69e20109af73",
   "metadata": {},
   "source": [
    "run_test_and_plot(model, best_ckpt_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ddb4e27e-1e7f-4423-99eb-ffdae7ebf9d7",
   "metadata": {},
   "source": [
    "best_ckpt_100_epoch_path = \"multicrop_best-epoch=76.ckpt\"\n",
    "\n",
    "if not os.path.isfile(best_ckpt_100_epoch_path):\n",
    "    gdown.download(\"https://drive.google.com/uc?id=1cO5a9PmV70j6mvlTc8zH8MnKsRCGbefm\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "35a77263-5308-4781-a17f-a35e62ca1875",
   "metadata": {
    "id": "35a77263-5308-4781-a17f-a35e62ca1875",
    "scrolled": true
   },
   "source": [
    "run_test_and_plot(model, best_ckpt_100_epoch_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a0c88e2d5ab78020",
   "metadata": {},
   "source": [
    "# Fine-tuning via CLI\n",
    "\n",
    "You might want to restart the session to free up GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "id": "bdbf05ebc81b9998",
   "metadata": {},
   "source": [
    "# Run fine-tuning\n",
    "!terratorch fit -c \"../configs/prithvi_v2_eo_300_tl_unet_multitemporal_crop.yaml\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To run this via terminal:\n",
    "1. Open terminal from the jupyterlab home page\n",
    "2. Activate conda `source /opt/conda/bin/activate`\n",
    "3. Activate appropriate conda environment `conda activate prithvi_eo`\n",
    "4. Navigate to the notebook directory: `cd \"/home/sagemaker-user/ESA-NASA-workshop-2025/Track 1 (EO)/TerraMind/notebooks/\"`\n",
    "5. run terratorch training script: `terratorch fit -c \"../configs/terramind_v1_base_sen1floods11.yaml\"`"
   ],
   "id": "a29fb74478837d3"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
