{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PrithviWxC Rollout Inference\n",
    "If you haven't already, take a look at the exmaple for the PrithviWxC core\n",
    "model, as we will pass over the points covered there.\n",
    "\n",
    "Here we will introduce the PrithviWxC model that was trained furhter for\n",
    "autoregressive rollout, a common strategy to increase accuracy and stability of\n",
    "models when applied to forecasting-type tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from huggingface_hub import hf_hub_download, snapshot_download\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the variable true if you want to use the smaller model, else set it to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_small_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set backend etc.\n",
    "torch.jit.enable_onednn_fusion(True)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Set seeds\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Set variables\n",
    "surface_vars = [\n",
    "    \"EFLUX\",\n",
    "    \"GWETROOT\",\n",
    "    \"HFLUX\",\n",
    "    \"LAI\",\n",
    "    \"LWGAB\",\n",
    "    \"LWGEM\",\n",
    "    \"LWTUP\",\n",
    "    \"PS\",\n",
    "    \"QV2M\",\n",
    "    \"SLP\",\n",
    "    \"SWGNT\",\n",
    "    \"SWTNT\",\n",
    "    \"T2M\",\n",
    "    \"TQI\",\n",
    "    \"TQL\",\n",
    "    \"TQV\",\n",
    "    \"TS\",\n",
    "    \"U10M\",\n",
    "    \"V10M\",\n",
    "    \"Z0M\",\n",
    "]\n",
    "static_surface_vars = [\"FRACI\", \"FRLAND\", \"FROCEAN\", \"PHIS\"]\n",
    "vertical_vars = [\"CLOUD\", \"H\", \"OMEGA\", \"PL\", \"QI\", \"QL\", \"QV\", \"T\", \"U\", \"V\"]\n",
    "levels = [\n",
    "    34.0,\n",
    "    39.0,\n",
    "    41.0,\n",
    "    43.0,\n",
    "    44.0,\n",
    "    45.0,\n",
    "    48.0,\n",
    "    51.0,\n",
    "    53.0,\n",
    "    56.0,\n",
    "    63.0,\n",
    "    68.0,\n",
    "    71.0,\n",
    "    72.0,\n",
    "]\n",
    "padding = {\"level\": [0, 0], \"lat\": [0, -1], \"lon\": [0, 0]}\n",
    "\n",
    "variable_names = surface_vars + [\n",
    "    f'{var}_level_{level}' for var in vertical_vars for level in levels\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lead time\n",
    "When performing auto-regressive rollout, the intermediate steps require the\n",
    "static data at those times and---if using `residual=climate`---the intermediate\n",
    "climatology. We provide a dataloader that extends the MERRA2 loader of the\n",
    "core model, adding in these additional terms. Further, it return target data for\n",
    "the intermediate steps if those are required for loss terms. \n",
    "\n",
    "The `lead_time` flag still lets the target time for the model, however now it\n",
    "only a single value and must be a positive integer multiple of the `-input_time`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lead_time = 12  # This variable can be change to change the task\n",
    "input_time = -6  # This variable can be change to change the task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data file\n",
    "MERRA-2 data is available from 1980 to the present day,\n",
    "at 3-hour temporal resolution. The dataloader we have provided\n",
    "expects the surface data and vertical data to be saved in\n",
    "separate files, and when provided with the directories, will\n",
    "search for the relevant data that falls within the provided time range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_range = (\"2020-09-01T00:00:00\", \"2020-09-10T23:59:59\")\n",
    "surf_dir = Path(\"/rgroup/aifm/ankur_hurricane_data/processed_data_sfc\")\n",
    "vert_dir = Path(\"/rgroup/aifm/ankur_hurricane_data/merra_pressuredata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Climatology\n",
    "The PrithviWxC model was trained to calculate the output by\n",
    "producing a perturbation to the climatology at the target time.\n",
    " This mode of operation is set via the `residual=climate` option.\n",
    " This was chosen as climatology is typically a strong prior for\n",
    " long-range prediction. When using the `residual=climate` option,\n",
    " we have to provide the dataloader with the path of the\n",
    " climatology data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surf_clim_dir = Path(\"/rgroup/aifm/ankur_hurricane_data/MERRA2_v2_Statistics/surface_level\")\n",
    "vert_clim_dir = Path(\"/rgroup/aifm/ankur_hurricane_data/MERRA2_v2_Statistics/pressure_level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positional_encoding = \"fourier\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader init\n",
    "We are now ready to instantiate the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PrithviWxC.dataloaders.merra2_rollout import Merra2RolloutDataset\n",
    "\n",
    "dataset = Merra2RolloutDataset(\n",
    "    time_range=time_range,\n",
    "    lead_time=lead_time,\n",
    "    input_time=input_time,\n",
    "    data_path_surface=surf_dir,\n",
    "    data_path_vertical=vert_dir,\n",
    "    climatology_path_surface=surf_clim_dir,\n",
    "    climatology_path_vertical=vert_clim_dir,\n",
    "    surface_vars=surface_vars,\n",
    "    static_surface_vars=static_surface_vars,\n",
    "    vertical_vars=vertical_vars,\n",
    "    levels=levels,\n",
    "    positional_encoding=positional_encoding,\n",
    ")\n",
    "assert len(dataset) > 0, \"There doesn't seem to be any valid data.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "### Scalers and other hyperparameters\n",
    "Again, this setup is similar as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PrithviWxC.dataloaders.merra2 import (\n",
    "    input_scalers,\n",
    "    output_scalers,\n",
    "    static_input_scalers,\n",
    ")\n",
    "surf_in_scal_path = Path(\"/rgroup/aifm/ankur_hurricane_data/MERRA2_v2_Statistics/surface_level/musigma_surface.nc\")\n",
    "vert_in_scal_path = Path(\"/rgroup/aifm/ankur_hurricane_data/MERRA2_v2_Statistics/pressure_level/musigma_vertical.nc\")\n",
    "surf_out_scal_path = Path(\"/rgroup/aifm/ankur_hurricane_data/MERRA2_v2_Statistics/surface_level/anomaly_variance_surface.nc\")\n",
    "vert_out_scal_path = Path(\"/rgroup/aifm/ankur_hurricane_data/MERRA2_v2_Statistics/pressure_level/anomaly_variance_vertical.nc\")\n",
    "\n",
    "\n",
    "\n",
    "in_mu, in_sig = input_scalers(\n",
    "    surface_vars,\n",
    "    vertical_vars,\n",
    "    levels,\n",
    "    surf_in_scal_path,\n",
    "    vert_in_scal_path,\n",
    ")\n",
    "\n",
    "output_sig = output_scalers(\n",
    "    surface_vars,\n",
    "    vertical_vars,\n",
    "    levels,\n",
    "    surf_out_scal_path,\n",
    "    vert_out_scal_path,\n",
    ")\n",
    "\n",
    "static_mu, static_sig = static_input_scalers(\n",
    "    surf_in_scal_path,\n",
    "    static_surface_vars,\n",
    ")\n",
    "\n",
    "residual = \"climate\"\n",
    "masking_mode = \"both\"\n",
    "encoder_shifting = True\n",
    "decoder_shifting = True\n",
    "masking_ratio = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model init\n",
    "We can now build and load the pretrained weights, note that you should use the\n",
    "rollout version of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_small_model:\n",
    "    weights_path = Path(\"/rgroup/aifm/rohit/Prithvi-WxC/examples/consolidated.pth\")\n",
    "    with open(\"../data/config_small.yaml\", \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "else:\n",
    "    weights_path = Path(\"/rtmp/rlal/debug/step_20_ckp.pth\")\n",
    "    with open(\"../data/config.yaml\", \"r\") as f:\n",
    "        config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PrithviWxC.model import PrithviWxC\n",
    "\n",
    "\n",
    "model = PrithviWxC(\n",
    "    in_channels=config[\"params\"][\"in_channels\"],\n",
    "    input_size_time=config[\"params\"][\"input_size_time\"],\n",
    "    in_channels_static=config[\"params\"][\"in_channels_static\"],\n",
    "    input_scalers_mu=in_mu,\n",
    "    input_scalers_sigma=in_sig,\n",
    "    input_scalers_epsilon=config[\"params\"][\"input_scalers_epsilon\"],\n",
    "    static_input_scalers_mu=static_mu,\n",
    "    static_input_scalers_sigma=static_sig,\n",
    "    static_input_scalers_epsilon=config[\"params\"][\n",
    "        \"static_input_scalers_epsilon\"\n",
    "    ],\n",
    "    output_scalers=output_sig**0.5,\n",
    "    n_lats_px=config[\"params\"][\"n_lats_px\"],\n",
    "    n_lons_px=config[\"params\"][\"n_lons_px\"],\n",
    "    patch_size_px=config[\"params\"][\"patch_size_px\"],\n",
    "    mask_unit_size_px=config[\"params\"][\"mask_unit_size_px\"],\n",
    "    mask_ratio_inputs=masking_ratio,\n",
    "    mask_ratio_targets=0.0,\n",
    "    embed_dim=config[\"params\"][\"embed_dim\"],\n",
    "    n_blocks_encoder=config[\"params\"][\"n_blocks_encoder\"],\n",
    "    n_blocks_decoder=config[\"params\"][\"n_blocks_decoder\"],\n",
    "    mlp_multiplier=config[\"params\"][\"mlp_multiplier\"],\n",
    "    n_heads=config[\"params\"][\"n_heads\"],\n",
    "    dropout=config[\"params\"][\"dropout\"],\n",
    "    drop_path=config[\"params\"][\"drop_path\"],\n",
    "    parameter_dropout=config[\"params\"][\"parameter_dropout\"],\n",
    "    residual=residual,\n",
    "    masking_mode=masking_mode,\n",
    "    encoder_shifting=encoder_shifting,\n",
    "    decoder_shifting=decoder_shifting,\n",
    "    positional_encoding=positional_encoding,\n",
    "    checkpoint_encoder=[],\n",
    "    checkpoint_decoder=[],\n",
    ")\n",
    "\n",
    "\n",
    "state_dict = torch.load(weights_path, weights_only=False)\n",
    "if \"model_state\" in state_dict:\n",
    "    state_dict = state_dict[\"model_state\"]\n",
    "model.load_state_dict(state_dict, strict=True)\n",
    "\n",
    "if (hasattr(model, \"device\") and model.device != device) or not hasattr(\n",
    "    model, \"device\"\n",
    "):\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollout\n",
    "We are now ready to perform the rollout. Agin the data has to be run through a\n",
    "preprocessor. However this time we use a preprocessor that can handle the\n",
    "additional intermediate data. Also, rather than calling the model directly, we\n",
    "have a conveient wrapper function that performs the interation. This also\n",
    "simplifies the model loading when using a sharded cahckpoint. If you attempt to\n",
    "perform training steps upton this function, we should use an aggressive number\n",
    "of activation checkpoints as the memory consumption becomes quite high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PrithviWxC.dataloaders.merra2_rollout import preproc\n",
    "from PrithviWxC.rollout import rollout_iter\n",
    "\n",
    "data = next(iter(dataset))\n",
    "batch = preproc([data], padding)\n",
    "\n",
    "for k, v in batch.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        batch[k] = v.to(device)\n",
    "\n",
    "rng_state_1 = torch.get_rng_state()\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    out = rollout_iter(dataset.nsteps, model, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2m = out[0, variable_names.index(\"T2M\")].cpu().numpy()\n",
    "\n",
    "lat = np.linspace(-90, 90, out.shape[-2])\n",
    "lon = np.linspace(-180, 180, out.shape[-1])\n",
    "X, Y = np.meshgrid(lon, lat)\n",
    "\n",
    "plt.contourf(X, Y, t2m, 100)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u10m = out[0, variable_names.index(\"U10M\")].cpu().numpy()\n",
    "\n",
    "lat = np.linspace(-90, 90, out.shape[-2])\n",
    "lon = np.linspace(-180, 180, out.shape[-1])\n",
    "X, Y = np.meshgrid(lon, lat)\n",
    "\n",
    "plt.contourf(X, Y, u10m, 100)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
