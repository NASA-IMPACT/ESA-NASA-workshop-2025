{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Use Case Demonstration of EVE\n",
    "In this notebook, we explore practical applications of EVE (Earth Virtual Explorer), a large language model specialized in Earth Observation (EO). EVE is designed to understand, analyze, and generate text related to EO data and topics, making it a valuable tool for researchers, analysts, and decision-makers in the field.\n",
    "\n",
    "We will demonstrate two key use cases of EVE:\n",
    "\n",
    "- **Summarization**: In this task, EVE is given a document related to Earth Observation and asked to generate a concise and informative summary. This is useful for quickly understanding lengthy reports, scientific papers, or satellite data documentation.\n",
    "\n",
    "- **Question Answering (Q&A)**: In this use case, we enhance EVE’s performance by integrating it with a retrieval system, using a technique known as Retrieval-Augmented Generation (RAG). Here, the model first retrieves relevant context from a knowledge base before answering questions, leading to more accurate and grounded responses.\n",
    "\n",
    "These examples highlight EVE’s potential to streamline information processing and support decision-making in Earth Observation workflows.\n",
    "\n",
    "Before diving into the use case, let’s briefly explore the core idea behind Large Language Models (LLM) and Natural Language Processing (NLP)."
   ],
   "metadata": {
    "id": "qh_1LMP-T5-E"
   },
   "id": "qh_1LMP-T5-E"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Natural Language Processing\n",
    "NLP is a field of linguistics and machine learning focused on understanding everything related to human language. The aim of NLP tasks is not only to understand single words individually, but to be able to understand the context of those words. Some classic tasks in NLP are the following\n",
    "- Classifying whole sentences: Getting the sentiment of a review, detecting if an email is spam, determining if a sentence is grammatically correct or whether two sentences are logically related or not\n",
    "- Classifying each word in a sentence: Identifying the grammatical components of a sentence (noun, verb, adjective), or the named entities (person, location, organization)\n",
    "- Generating text content: Completing a prompt with auto-generated text, filling in the blanks in a text with masked word\n",
    "- ..."
   ],
   "metadata": {
    "id": "6LAVsV0Vxtmy"
   },
   "id": "6LAVsV0Vxtmy"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Install the required libraries\n",
    "!pip3 install -q -U bitsandbytes\n",
    "!pip3 install -q datasets\n",
    "!pip3 install -q langchain_community\n",
    "!pip3 install -q pypdf\n",
    "!pip3 install -q sentence-transformers\n",
    "!pip3 install -q faiss-cpu\n",
    "!pip3 install -q langchain_huggingface\n",
    "!pip3 install -q langchain_runpod\n",
    "!pip3 install -q qdrant_client\n",
    "!pip3 install -q langchain_aws\n",
    "!pip3 install numpy==1.26.4\n"
   ],
   "id": "211728544911d40e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Large Language Models (LLM)\n",
    "In recent years, the field of NLP has been revolutionized by Large Language Models (LLMs).\n",
    "LLM are a powerful subset of NLP models characterized by their massive size, extensive training data, and ability to perform a wide range of language tasks with minimal task-specific training.\n",
    "\n",
    "The objective of an LLM and more generally of a Language Model is to compute the probability of a series of words/tokens\n",
    "\n",
    "1. **Chain rule**: probability of a sequence is the product of conditional probabilities\n",
    "$$P\\left(w_1^m\\right)=\\prod_{n=1}^m P\\left(w_n \\mid w_1^{n-1}\\right)$$\n",
    "2. **Markov assumption**: N-gram model approximates it by conditioning only on the last N−1 words:\n",
    "\n",
    "$$P\\left(w_n \\mid w_1^{n-1}\\right) \\approx P\\left(w_n \\mid w_{n-N+1}^{n-1}\\right)$$\n",
    "\n",
    "So in general for **N-grams**\n",
    "$$P\\left(w_1^m\\right) \\approx P\\left(w_1^N\\right) \\cdot \\prod_{n=N+1}^M P\\left(w_n \\mid w_{n-N+1}^{n-1}\\right)$$\n",
    "\n",
    "**Example** for trigrams (N=3)\n",
    "$$P(\\text { country roads }) \\approx P(\\text { cou }) \\cdot P(n \\mid o u) \\cdot P(t \\mid u n) \\cdot P(r \\mid n t) \\cdot P(y \\mid t r) \\cdot P(\\mid r y) \\cdot \\text { etc. }$$\n",
    "\n",
    "Now let's load a LLM from HuggingFace"
   ],
   "metadata": {
    "id": "gAPWwur7yAZs"
   },
   "id": "gAPWwur7yAZs"
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('TinyLlama/TinyLlama-1.1B-Chat-v1.0')"
   ],
   "metadata": {
    "id": "sW_vjEG63aAF"
   },
   "id": "sW_vjEG63aAF",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before feeding our LLM with our question we need to tokenize the question. The tokenization is the process of breaking into pieces a text, this pieces are called token. Subword Tokenization is one of the most popular and efficent tokenization methods,  a word is split into subwords and these subwords are known as tokens, For example the word “football” might be split into “foot”, and “ball”.\n",
    "\n",
    "Let's load a tokenizer and see how it works.\n",
    ""
   ],
   "metadata": {
    "id": "6ILU6EGo5-jS"
   },
   "id": "6ILU6EGo5-jS"
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('TinyLlama/TinyLlama-1.1B-Chat-v1.0')\n",
    "\n",
    "# Tokenize with options to return offsets and word_ids\n",
    "encoded = tokenizer(\n",
    "    question,\n",
    "    return_offsets_mapping=True,\n",
    "    return_tensors=None,\n",
    "    add_special_tokens=True\n",
    ")\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded[\"input_ids\"])\n",
    "# The _ denotes a space before the word\n",
    "print(tokens)"
   ],
   "metadata": {
    "id": "QCTkVNCQ7Jyy"
   },
   "id": "QCTkVNCQ7Jyy",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prompting\n",
    "Prompting is the process of providing input to a language model in the form of a carefully designed instruction or question to guide its output.\n",
    "\n",
    "First, we will define the prompt to be used using three different parts:\n",
    "- **System message**: this section provides guidelines for the model on how to behave and interpret the conversation.\n",
    "- **Human message**: this section contains the input or message from the user.\n",
    "- **AI message**: this section represents a response generated by the model.\n",
    "\n",
    "From the code below we can see the structure of the prompt and the templates used to create it. Specifically,. Specifically we could see some **special tokens** used in the prompt:\n",
    "- **<|system|> | <|user|> | <|assistant|>**: are special tokens that helps the model to understand to who belongs that specific message.\n",
    "- **<|end|>**: is a special token that indicates the end of a message.\n",
    "- **{message}**: is a placeholder that will be replaced with the actual message, context and question."
   ],
   "metadata": {
    "id": "kbxBwRpTGpxU"
   },
   "id": "kbxBwRpTGpxU"
  },
  {
   "cell_type": "code",
   "source": [
    "prompt = \"\"\"<|system|>\n",
    "You are a helpful chatbot, be kind and answer to the user questions.<|end|>\n",
    "<|user|>\n",
    "{message}\n",
    "<|end|>\n",
    "<|assistant|>\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "c2dDXdqXH9g2"
   },
   "id": "c2dDXdqXH9g2",
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "message = '' # Your message here\n",
    "prompt = prompt.format(message=message)"
   ],
   "metadata": {
    "id": "cLDuAq0oIlR7"
   },
   "id": "cLDuAq0oIlR7",
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "# Let's assemble our pipeline\n",
    "# Build the text generation pipeline\n",
    "llama_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device_map=\"auto\"  # uses GPU if available\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CfoEVD0bDDX3",
    "outputId": "a4a81c7a-6c2f-4793-f297-57af83acc6cd"
   },
   "id": "CfoEVD0bDDX3",
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "output = llama_pipe(prompt, max_new_tokens=1000, do_sample=True, temperature=0.7)"
   ],
   "metadata": {
    "id": "MMbhcJURDyrc"
   },
   "id": "MMbhcJURDyrc",
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Here we can see the whole conversation with the generated answer by the model\n",
    "print(output[0]['generated_text'])"
   ],
   "metadata": {
    "id": "GNFuyMRvD4fg"
   },
   "id": "GNFuyMRvD4fg",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summarization\n",
    "\n",
    "In this section, we will ask our model to perform text summarization giving as a document a paper from our dataset."
   ],
   "metadata": {
    "id": "udcAFT-wWzlc"
   },
   "id": "udcAFT-wWzlc"
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import  load_dataset\n",
    "import random\n",
    "# Load documents\n",
    "# TODO - replace with from a document within the public dataset\n",
    "docs = load_dataset('eve-esa/eve-cpt-sample-v0.2')['train']\n",
    "\n",
    "# Select a random doc from the dataset\n",
    "doc = docs.select(random.sample(range(len(docs)), 1))[0]"
   ],
   "metadata": {
    "id": "ZkmZqeqPWuEF"
   },
   "id": "ZkmZqeqPWuEF",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_aws import BedrockLLM\n",
    "\n",
    "# Load the model using APIs\n",
    "llm = BedrockLLM(model_id='arn:aws:bedrock:us-west-2:637423382292:imported-model/7i06g1utels3', region_name='us-west-2', provider='meta')"
   ],
   "metadata": {
    "id": "Nu1eHm5tXoq_"
   },
   "id": "Nu1eHm5tXoq_",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Prompt message\n",
    "\n",
    "message = f\"\"\"\n",
    "<|system|>\n",
    "You are an helpful assistant expert in Earth Observation, help the user with his tasks.\n",
    "<|end|>\n",
    "<|user|>\n",
    "Summarize the following document focusing on the main concepts and ideas.\n",
    "\n",
    "The document starts here:\n",
    "{doc['text']}\n",
    "\n",
    "<|end|>\n",
    "<|assistant|>\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "id": "uSYFscIPfgSM"
   },
   "id": "uSYFscIPfgSM",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Run this cell to show the final prompt given to the model\n",
    "print(message)"
   ],
   "metadata": {
    "id": "wI7y0uBl0nDP"
   },
   "id": "wI7y0uBl0nDP",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "output = llm.invoke(message)"
   ],
   "metadata": {
    "id": "tKyDu4JKedrX"
   },
   "id": "tKyDu4JKedrX",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "output"
   ],
   "metadata": {
    "id": "6B6G3af6ciOM"
   },
   "id": "6B6G3af6ciOM",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "6c07c15e489c973a"
   },
   "cell_type": "markdown",
   "source": [
    "# Retrieval Augmented Generation (RAG) with Langchain\n",
    "\n",
    "In this notebook, we implement a complete RAG pipeline for answering questions based on a given context. Using the LangChain library, we'll walk through the entire process—from retrieving relevant context to generating accurate answers.\n"
   ],
   "id": "6c07c15e489c973a"
  },
  {
   "metadata": {
    "id": "b793ab84cc858c7f"
   },
   "cell_type": "markdown",
   "source": [
    "**Roadmap**\n",
    "\n",
    "1. **Indexing**: Organize the raw documents into a structured format suitable for processing, such as splitting them into chunks or passages for more efficient retrieval.\n",
    "\n",
    "2. **Embedding**: Convert each text chunk into a dense vector representation using a pre-trained embedding model. These embeddings capture the semantic meaning of the content.\n",
    "\n",
    "3. **Vector Store**: Store the embeddings in a vector database (Qdrant in our case), allowing fast and scalable similarity search across the document collection.\n",
    "\n",
    "4. **Retrieval and Generation**: Given a user query, retrieve the most relevant document chunks from the vector store and feed them into a language model (EVE) to generate a context-aware, accurate response."
   ],
   "id": "b793ab84cc858c7f"
  },
  {
   "metadata": {
    "id": "6f19179e8af8ce6a"
   },
   "cell_type": "markdown",
   "source": [
    "## Load dataset of Q&A\n",
    "Let's load our dataset of Q&A about EO, each sample is composed of a question and an answer"
   ],
   "id": "6f19179e8af8ce6a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T10:21:01.031700Z",
     "start_time": "2025-03-24T10:20:56.045973Z"
    },
    "id": "b733b528b619214"
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "qa = load_dataset('eve-esa/eve-is-open-ended')['train']\n",
    "\n",
    "idx = 120\n",
    "\n",
    "question = qa[idx]['question']\n",
    "answer = qa[idx]['answer']\n",
    "\n",
    "\n",
    "print('Question: ', question)\n",
    "print('Answer: ', answer)"
   ],
   "id": "b733b528b619214",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "1fe9e4d1eed5cb3"
   },
   "cell_type": "markdown",
   "source": [
    "## Indexing\n",
    "\n",
    "The first part of a RAG pipeline is called **indexing**. This is the process of ingesting data from a source and indexing it. The indexing process is composed of three steps:\n",
    "- **Load**: process and load data in text format.\n",
    "- **Split**: this is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\n",
    "- **Store**: we need somewhere to store and index our splits, so that they can be searched over later. This is often done using a [VectorStore](https://python.langchain.com/docs/concepts/vectorstores/) and [Embeddings](https://python.langchain.com/docs/concepts/embedding_models/) model.\n",
    "Once the Indexing step is done we will have our knowledge base made of scientific papers indexed and ready to be used in the generation steps as context.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"https://python.langchain.com/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png\" width=\"800\"/>\n",
    "</div>\n"
   ],
   "id": "1fe9e4d1eed5cb3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Chunking is a fundamental step in working with large language models, especially in retrieval-augmented generation (RAG) pipelines. It refers to the process of splitting long documents or texts into smaller, manageable pieces—called chunks—that can be efficiently stored, indexed, and retrieved when needed. Well-formed chunks are crucial because they ensure that each piece of text contains enough context to be meaningful on its own, which in turn improves the quality of retrieval and the relevance of the information provided to the language model. Poorly chunked text can lead to incomplete or confusing results, so careful design of the chunking strategy is essential.\n"
   ],
   "metadata": {
    "id": "WA8RIU9ZLK2f"
   },
   "id": "WA8RIU9ZLK2f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we are working with structured documents, we want to avoid blindly splitting the text based solely on character or token length. Instead, we can take advantage of the document’s markdown hierarchy to guide our chunking strategy. By doing so, each chunk will correspond to a logical section of the document—such as a heading and its associated content—making the chunks more coherent and meaningful. This structure-aware approach improves the quality of retrieval and the relevance of the context passed to the language model. We will use LangChain to implement this custom splitting logic efficiently."
   ],
   "metadata": {
    "id": "NiD0_W7tM1V6"
   },
   "id": "NiD0_W7tM1V6"
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"####\", \"Header 4\"),\n",
    "    (\"#####\", \"Header 5\"),\n",
    "    (\"######\", \"Header 6\")\n",
    "]\n",
    "\n",
    "# TODO - replace with from a document within the public dataset\n",
    "markdown_document = \"\"\"###### Abstract\n",
    "\n",
    "Total nitrogen concentration (C\\({}_{\\text{TN}}\\)) enrichment is the primary cause of natural water eutrophication. Accurately estimating C\\({}_{\\text{TN}}\\) and its spatiotemporal dynamics is crucial for formulating monitoring and control measures to alleviate lake eutrophication. A hybrid model was proposed for estimating C\\({}_{\\text{TN}}\\) in optically complex inland waters by incorporating the relationship between C\\({}_{\\text{TN}}\\) and water optical active components for Zhuhai-1 Orbita hyperspectral (OHS) imagery. Compared with other semi-analytical algorithms, the re-adjusted reference wavelength QMA's shows the best performance in a(a) retrieval. The hybrid model for C\\({}_{\\text{TN}}\\) estimation achieves an root mean square deviation (RMSD) of 0.20 mg/L, a mean absolute percentage deviation (MAPD) of 6.96 %, and a unbiased mean absolute percentage deviation (UMAPD) of 6.96 %, with a\\({}_{\\text{DM}}\\)(569) accuracy exerting the greatest influence. Ground-satellite synchronous validation demonstrates robust performance, with an RMSD of 0.28 mg/L, a MAPD of 14.49 %, and a UMAPD of 14.92 %. The hybrid model was applied to OHS observations of Lake Dinch from April 2019 to September 2021. The analysis revealed a generally decreasing trend in C\\({}_{\\text{TN}}\\) during this timeframe. The above results demonstrate that the robustness and applicability of the proposed C\\({}_{\\text{TN}}\\) hybrid model for inland waters with complex optical properties. Furthermore, satellite-based data products provide valuable information for formulating lake management strategies.\n",
    "\n",
    "## 1 Introduction\n",
    "\n",
    "Freshwater lakes serve as crucial ecosystems that significantly influence global climate regulation, maintain biodiversity, and provide essential resources for humans (Ho et al., 2019; Li et al., 2024). Human activities, including agricultural runoff and industrial waste discharge, coupled with the effects of global climate change, are causing widespread eutrophication and shrinkage of lakes worldwide (Dai et al., 2023; Grant et al., 2021). Excessive nitrogen elements in aquatic ecosystems have severe adverse effects on aquatic organisms and humans, such as eutrophication, water acidification, and toxicity (Li et al., 2023; Sheikholeslami and Hall, 2023). Therefore, obtaining total nitrogen concentration (C\\({}_{\\text{TN}}\\)) and its spatial distribution is critical for furthering our understanding of the eutrophication process and biogeochemicalprocess of aquatic ecosystems.\n",
    "\n",
    "Lake Bianchi is the the largest freshwater lake in Yunnan Province of China. It plays a crucial role in the region's ecological balance, water supply, and economic development (Li et al., 2024; Zheng et al., 2023). However, like other reservoirs and lakes around the world, the lake has suffered from severe environmental degradation in recent decades (Feng et al., 2021; Ho et al., 2019). Since the 1970 s, Lake Bianchi has experienced prolonged algal blooms due to increased nitrogen and phosphorus loads, severely affecting the lake's aquatic ecology and the likelihoods of surrounding residents (Cheng et al., 2023; Mu et al., 2021). Therefore, monitoring nutrient concentrations, especially total nitrogen, is crucial for the management of lake eutrophication.\n",
    "\n",
    "Traditionally, \\(\\mathrm{C_{IN}}\\) monitoring and assessment have relied on field measurements and laboratory analysis (Wang et al., 2022). While these methods provide valuable data points, they are inherently limited in their ability to capture large-scale spatial and temporal variations in \\(\\mathrm{C_{IN}}\\). Additionally, they are both labor-intensive and time-consuming to implement (Cai et al., 2023; Li et al., 2024). Earth observation techniques, with their wide spatial coverage and capability for long-term, continuous monitoring, offer significant potential for uninterrupted water quality monitoring (Liu et al., 2020; Zheng et al., 2023). However, accurately estimating \\(\\mathrm{C_{IN}}\\) remains challenging. Unlike optically active substances (OACs), \\(\\mathrm{C_{IN}}\\) lacks a distinct spectral signature within the detectable wavelength range, and TN is therefore referred to as a non-optically active substance (Li et al., 2023; Li et al., 2017). Therefore, estimating \\(\\mathrm{C_{IN}}\\) in inland entropic waters using the spectral features of remote sensing is difficult.\n",
    "\n",
    "In the last two decades, researchers have achieved substantial advancements in \\(\\mathrm{C_{IN}}\\) estimation using remote sensing techniques, primarily through direct and indirect estimation models (Chen and Quan, 2012; Dong et al., 2020; Guo et al., 2021; Guo et al., 2022; Li et al., 2023; Li et al., 2017; Qun'ou et al., 2021; Torbick et al., 2013; Wang et al., 2020; Zhu et al., 2023). Direct estimation models establish statistical relationships between remote sensing reflectance (\\(\\mathrm{R_{\\mathrm{\\SIUnitSymbolMicro m}}}\\)) and \\(\\mathrm{C_{IN}}\\). Multiple linear regression models have been employed for estimating \\(\\mathrm{C_{IN}}\\) in various water bodies, such as Lake Taihu, the Pearl River Estuary, and the Lower Peninsula of Michigan (Chen and Quan, 2012; Guo et al., 2022; Torbick et al., 2013). Beyond traditional methods, researchers have increasingly focused on leveraging machine learning algorithms for \\(\\mathrm{C_{IN}}\\) estimation. Examples include artificial neural networks, multi-spectral scale morphological combined features, support vector machines, AdaBoost Regression, and Gradient Boosting Regression, which have been trained and successfully applied to \\(\\mathrm{C_{IN}}\\) estimation in inland rivers and lakes (Guo et al., 2021; Li et al., 2023; Zhu et al., 2023). While direct retrieval models offer advantages in terms of simplicity and ease of use, their effectiveness and applicability across diverse water bodies are significantly constrained by limitations in data availability and the complexity of the relationship between \\(\\mathrm{C_{IN}}\\) and the selected features (Cai et al., 2023). Additionally, the direct use of \\(\\mathrm{R_{\\mathrm{\\SIUnitSymbolMicro m}}}\\)(\\(\\mathrm{\\SIUnitSymbolMicro}\\)) to estimate \\(\\mathrm{C_{IN}}\\) is controversial, as TN is a non-optically active substance, and changes in \\(\\mathrm{C_{IN}}\\) are difficult to reflect in \\(\\mathrm{R_{\\mathrm{\\SIUnitSymbolMicro m}}}\\)(Li et al., 2023). The second type involves indirect estimation models that leverage the relationships between \\(\\mathrm{C_{IN}}\\) and OACs. Chen and Quan (2012) constructed a multiple linear regression model using OACs as predictor variables and \\(\\mathrm{C_{IN}}\\) as the response variable. This model is applicable to \\(\\mathrm{C_{IN}}\\) estimation in Lake Taihu. Previous empirical models only used band ratios or band combination models to estimate \\(\\mathrm{C_{IN}}\\), and such models are difficult to directly apply to different lakes or different periods within the same lake (Oyama et al., 2009). Compared to direct estimation models, indirect models offer the advantage of incorporating bio-optical relationships between \\(\\mathrm{C_{IN}}\\) and OACs. Therefore, we attempted to develop a novel \\(\\mathrm{C_{IN}}\\) estimation model that considers the relationship between \\(\\mathrm{C_{IN}}\\) and the inherent optical properties of water bodies.\n",
    "\n",
    "In optically complex waters, various constituents like phytoplankton, suspended solids, and colored dissolved organic matter (CDOM) collectively modulate the water's spectral signature, making it challenging to isolate the signal specific to \\(\\mathrm{C_{IN}}\\)(Xue et al., 2019). Additionally, the absorption coefficients of phytoplankton (\\(\\mathrm{a_{\\mathrm{\\SIUnitSymbolMicro m}}}\\)(\\(\\mathrm{\\SIUnitSymbolMicro}\\))), non-algal particles (\\(\\mathrm{a_{\\mathrm{\\SIUnitSymbolMicro m}}}\\)(\\(\\mathrm{\\SIUnitSymbolMicro}\\))), and colored dissolved organic matter (\\(\\mathrm{a_{\\mathrm{\\SIUnitSymbolMicro m}}}\\)(\\(\\mathrm{\\SIUnitSymbolMicro}\\))) quantify how these optically active constituents absorb light particles, thereby reducing the light energy that penetrates the water column. These coefficients serve as the connection between \\(\\mathrm{R_{\\mathrm{\\SIUnitSymbolMicro m}}}\\) and various OACs. Consequently, they are frequently incorporated as intermediary variables within models for estimating various water quality parameters (Liu et al., 2020; Zheng et al., 2023). Researchers have proposed various estimation models to estimate these absorption coefficients. These models encompass a range of approaches, including empirical approaches and semi-analytical approaches (Huang et al., 2014; Lee et al., 2014; Liu et al., 2020; Xue et al., 2019; Zheng et al., 2023). Building upon existing algorithms like QAA.V5, researchers have developed improved versions specifically designed for different water types. Lee et al. (2014) introduced QAA.V6 for clear ocean applications, utilizing a 670 nm reference wavelength and recalibrated coefficients. This model has proven effective in retrieving inherent optical properties (IOPs) of clear oceans (Jiang et al., 2019; Jorge et al., 2021). For inland turbid waters, modifications like Huang et al. (2014) obtain (using a 710 nm reference wavelength) and Xue et al. (2019) obtain \\(\\mathrm{QAA_{750}}\\) algorithm (using a 750 nm reference wavelength) were developed to address retrieval challenges in these environments. Zheng et al. (2023) proposed a QAA algorithm (\\(\\mathrm{QAA_{710}}\\)) with a reference wavelength of 716 nm and successfully estimated the total absorption coefficient (\\(\\mathrm{a_{\\mathrm{\\SIUnitSymbolMicro m}}}\\)) and \\(\\mathrm{a_{\\mathrm{\\SIUnitSymbolMicro m}}}\\)(\\(\\mathrm{\\SIUnitSymbolMicro}\\)) of Lake Tianchi. These advancements not only enabled IOP retrieval but also facilitated the successful estimation of chlorophyll-a (Chla) and total suspended matter (TSM), contributing to a more comprehensive assessment of inland water quality. Previous studies have demonstrated that using absorption coefficients, rather than OACs, is a more effective and feasible approach for estimating \\(\\mathrm{C_{IN}}\\) in optically complex inland waters (Shi et al., 2019; Zhang et al., 2018). Therefore, we attempted to develop a model based on \\(\\mathrm{C_{IN}}\\) and absorption coefficients.\n",
    "\n",
    "This study focused on developing a hybrid remote sensing algorithm specifically designed to estimate \\(\\mathrm{C_{IN}}\\) in inland waters with complex optical properties. The proposed hybrid algorithm integrates the improved QAA algorithm and semi-empirical algorithms to estimate absorption coefficients and their relationship with \\(\\mathrm{C_{IN}}\\). The Zhuhai-1 Orbita hyperspectral (OHS) satellite has emerged as a valuable asset for monitoring \\(\\mathrm{C_{IN}}\\) in inland waters. This next-generation satellite boasts exceptional spatial (10 m) and temporal resolution (2.5 days) alongside a high spectral resolution (32 spectral bands). This unique combination allows for the capture of fine-scale spatial and temporal variations in \\(\\mathrm{C_{IN}}\\) across inland aquatic ecosystems. Specifically, we took the following steps: (1) proposed a hybrid model for estimating \\(\\mathrm{C_{IN}}\\) based on absorption coefficients in inland waters; (2) employed the QAA\\({}_{716}\\) model to derive target absorption coefficients; (3) obtained the spatiotemporal variation of \\(\\mathrm{C_{IN}}\\) in Lake Tianchi, a typical eutrophic lake; (4) evaluated the feasibility of using OHS data to estimate \\(\\mathrm{C_{IN}}\\) and conduct a radiation performance assessment.\n",
    "\n",
    "## 2 Study area and data\n",
    "\n",
    "### Study region\n",
    "\n",
    "Lake Bianchi, the largest freshwater lake on the Yunnan-Guizhou Plateau, plays a vital role in the regional ecosystem and economy (Fig. 1). It covers an area of approximately 330 km\\({}^{2}\\) with a mean depth of 4.4 m (Huang et al., 2014). Lake Tianchi is a typical tectonic lake with numerous inflowing tributaries and only one outflowing river system, resulting in distinct closed-semi-closed characteristics. Over the past few decades, rapid urbanization and industrial development have led to a sharp deterioration in water quality, with large amounts of nitrogen and phosphorus nutrients being discharged into the lake (Li et al., 2023). This has resulted in increasing eutrophication of the lake,leading to large-scale algal blooms under suitable conditions, posing a serious threat to Kunming's tourism industry (Li et al., 2024).\n",
    "\n",
    "### Field data\n",
    "\n",
    "To capture seasonal variations in water quality and optical properties, field campaigns were conducted in Lake Dianchi during April and November 2017 (Fig. 1B & Table 1).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on)\n",
    "md_header_splits = markdown_splitter.split_text(markdown_document)\n"
   ],
   "metadata": {
    "id": "GfH_zVHPLJxS"
   },
   "id": "GfH_zVHPLJxS",
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for header_split in md_header_splits:\n",
    "  headers = \"\\n\".join([value for value in header_split.metadata.values()])\n",
    "  print(f'Section name: ', headers)\n",
    "  print('Chunk content:\\n', header_split.page_content)\n",
    "  print()"
   ],
   "metadata": {
    "id": "B7W0CyvUOC2V"
   },
   "id": "B7W0CyvUOC2V",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "1e6fe2ba335ad8fe"
   },
   "cell_type": "markdown",
   "source": [
    "### Embeddings\n",
    "\n",
    "An embeddings model in Retrieval-Augmented Generation (RAG) is a neural network that converts text into dense vector representations (embeddings) in a **high-dimensional space**. These models take text as input and produce a fixed-length array of numbers, a numerical fingerprint of the text's semantic meaning. Embeddings allow search system to find relevant documents not just based on keyword matches, but on semantic understanding.\n",
    "\n",
    "Embeddings models are trained on large text corpora using unsupervised learning techniques. They learn to encode the semantic meaning of words, sentences, and documents in a way that captures relationships between them. For example, embeddings models can learn that \"cat\" and \"dog\" are similar because they are both animals, or that \"apple\" and \"orange\" are similar because they are both fruits.\n",
    "\n",
    "There are many pre-trained embedding models available, each suited to different types of data and use cases. For our application, we use Indus, a fine-tuned encoder-only transformer model trained specifically on scientific journals and articles related to NASA’s Science Mission Directorate (SMD).\n",
    "\n",
    "Choosing the right embedding model is a critical step in building an effective retrieval system. Ideally, the embedding model should be trained—or at least fine-tuned—on data similar to the target documents. Since our corpus consists of scientific texts focused on Earth Observation, Indus is a better fit than a general-purpose model, as it captures domain-specific terminology and semantics more accurately.\n",
    "\n",
    "<img src=\"https://weaviate.io/assets/images/embedding-models-0c04d93c0be28dd63a0e8781c4e8685d.jpg\" width='800px'>\n",
    "\n",
    "\n"
   ],
   "id": "1e6fe2ba335ad8fe"
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load the embeddings model\n",
    "model_name = \"nasa-impact/nasa-smd-ibm-st-v2\"\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "indus_embd = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,  encode_kwargs=encode_kwargs\n",
    ")"
   ],
   "metadata": {
    "id": "XYz4pvs3jD7S"
   },
   "id": "XYz4pvs3jD7S",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "8746b6b79e3c4d83"
   },
   "cell_type": "markdown",
   "source": [
    "### Vector Store\n",
    "\n",
    "Vector stores are specialized databases designed to efficiently index and retrieve information using vector representations of data. Vector stores leverages the dense representation by reducing the task of finding similar documents to a search in a high-dimensional space. This search is made by comparing the vector representation of the **query** with the vector representation of the **documents** in the database. The documents that are closer to the query vector are considered more similar to the query.\n",
    "\n",
    "Wrapping up the retrieval process is composed of:\n",
    "- **Documents embedding**\n",
    "- **Store the embeddings in a VectorStore**\n",
    "- **Query embedding**\n",
    "- **Retrieve** the most similar documents to the query\n",
    "\n",
    "\n",
    "The most popular and simple setup is using the **cosine similarity** to compare the vectors and retrieve the **top k** most similar ones\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"https://python.langchain.com/assets/images/vectorstores-2540b4bc355b966c99b0f02cfdddb273.png\" width=\"800\"/>\n",
    "</div>\n"
   ],
   "id": "8746b6b79e3c4d83"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Connect to QDrant\n",
    "\n",
    "To save time, the embedding and indexing of documents have already been completed prior to this notebook. These steps can be computationally intensive, so we’ve pre-processed the data to streamline the workflow.\n",
    "\n",
    "We are using Qdrant as our vector store, which has been preloaded with all the relevant documents needed for retrieval. In this section, we will connect to the Qdrant instance and select the specific collection that contains our indexed data. This will enable us to perform efficient semantic searches and support the Retrieval-Augmented Generation (RAG) process used in our Q&A tasks."
   ],
   "metadata": {
    "id": "HWgZw1keeYtq"
   },
   "id": "HWgZw1keeYtq"
  },
  {
   "cell_type": "code",
   "source": [
    "# Examples of retrieval pipeline using the embedding function and the API from QDrant\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "qdrant_url = 'https://e186510c-4dd9-45c7-99a5-ae38c4c8bc36.us-east-1-0.aws.cloud.qdrant.io:6333'\n",
    "api_key = 'rZYblMkzsiqiiuPqxXxmckfyMFIZ9Yg9EpxYxhbeFZj82MEOIbT5Fg'\n",
    "\n",
    "# Enstablish a connection wit the vector store\n",
    "client = QdrantClient(\n",
    "    url=qdrant_url,\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# Embedd the query\n",
    "query_emb = indus_embd.embed_query(question)\n",
    "\n",
    "# Perform similarity search using the computed embeddings\n",
    "search_result = client.search(\n",
    "    collection_name=\"indus-test\",\n",
    "    query_vector=query_emb,\n",
    "    limit=1,\n",
    ")\n",
    "\n",
    "data = search_result[0].payload\n",
    "# Payload containing metadata and text\n",
    "for key, value in data['metadata'].items():\n",
    "  print(f'{key}: {value}')\n",
    "\n",
    "print('Retrieved chunk:\\n', data['page_content'])"
   ],
   "metadata": {
    "id": "OBzZdrljQXHv"
   },
   "id": "OBzZdrljQXHv",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from typing import List\n",
    "from qdrant_client import QdrantClient\n",
    "from pydantic import PrivateAttr, Field\n",
    "from typing import List, Optional, Dict\n",
    "from qdrant_client.models import Filter, PointStruct\n",
    "\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Let's define our retriever class to have a nice interface\n",
    "class RunpodRetriever():\n",
    "    def __init__(self, embedding, collection_name='indus-test', k: int = 3):\n",
    "        self._client = QdrantClient(url=\"https://e186510c-4dd9-45c7-99a5-ae38c4c8bc36.us-east-1-0.aws.cloud.qdrant.io:6333\",\n",
    "        api_key=\"rZYblMkzsiqiiuPqxXxmckfyMFIZ9Yg9EpxYxhbeFZj82MEOIbT5Fg\")\n",
    "        self.embedding = embedding\n",
    "        self.collection_name = collection_name\n",
    "        self.k = k\n",
    "\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        query_emb = self.embedding.embed_query(query)\n",
    "\n",
    "        search_result = self._client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            query_vector=query_emb,\n",
    "            limit=self.k,\n",
    "        )\n",
    "\n",
    "        docs = []\n",
    "        for hit in search_result:\n",
    "            # Adjust based on your actual data structure\n",
    "            data = hit.payload\n",
    "            content = data.get(\"page_content\", \"\")\n",
    "            metadata = data.get(\"metadata\", {})\n",
    "            docs.append(Document(page_content=content, metadata=metadata))\n",
    "\n",
    "        return docs\n"
   ],
   "metadata": {
    "id": "3ZqrzBMFbcw9"
   },
   "id": "3ZqrzBMFbcw9",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Let's define our retriever\n",
    "retriever = RunpodRetriever(indus_embd, k=3)\n",
    "\n",
    "# Format retrieved documents:\n",
    "def format_docs(docs):\n",
    "  doc_str = ''\n",
    "  for i, doc in enumerate(docs):\n",
    "    doc_str += f'Document n. {i+1}\\n'\n",
    "    doc_str += f'TITLE: {doc.metadata.get(\"source_name\", \"No title\")}\\n' # Add title's of the paper\n",
    "    doc_str += f'URL: {doc.metadata.get(\"source\", \"No url\")}\\n\\n' # Add URL of the paper\n",
    "    doc_str += f'{doc.page_content}\\n\\n'\n",
    "  return doc_str\n",
    "\n",
    "\n",
    "\n",
    "print('Question: ')\n",
    "print(question)\n",
    "print()\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "print(format_docs(docs))"
   ],
   "metadata": {
    "id": "ApDagHKExlZa",
    "ExecuteTime": {
     "end_time": "2025-03-24T10:49:48.506800Z",
     "start_time": "2025-03-24T10:49:46.230385Z"
    }
   },
   "id": "ApDagHKExlZa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "b019ad14ade15fcf"
   },
   "cell_type": "markdown",
   "source": [
    "## Retrieval and generation\n",
    "\n",
    "The pipeline consists of the following key components:\n",
    "\n",
    "- Retriever: This component queries the vector store (Qdrant) to fetch the most relevant document chunks based on the user’s question. It performs a semantic search using the pre-computed embeddings to find contextually similar content.\n",
    "\n",
    "- LLM (Large Language Model): Once the relevant context is retrieved, it is passed to EVE, our Earth Observation-specialized language model. EVE then generates a coherent and informed response based on both the query and the retrieved context.\n",
    "\n",
    "This approach ensures that the generated answers are grounded in the source documents, improving accuracy and reducing hallucination."
   ],
   "id": "b019ad14ade15fcf"
  },
  {
   "metadata": {
    "id": "a2e01ecdefdfa5a6"
   },
   "cell_type": "markdown",
   "source": [
    "### Prompt\n",
    "\n",
    "First, we will define the prompt to be used  using three different templates:\n",
    "- **SystemMessagePromptTemplate**: the system message represents guidelines for the model on how to interact with the user and interpret the conversation.\n",
    "- **AIMessagePromptTemplate**: the AI message represents a message generate by the model.\n",
    "- **HumanMessagePromptTemplate**: the human message represents the message sent by the user.\n"
   ],
   "id": "a2e01ecdefdfa5a6"
  },
  {
   "metadata": {
    "id": "18c906d0107239a5",
    "ExecuteTime": {
     "end_time": "2025-03-24T10:50:44.566443Z",
     "start_time": "2025-03-24T10:50:44.481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os  # Customize SystemPromptTemplate\n",
    "from langchain.prompts import SystemMessagePromptTemplate, AIMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "template= '''<|system|>\n",
    "{message}\n",
    "<|end|>\n",
    "'''\n",
    "\n",
    "# A human message will contain the question and the context. The context will be automatically added by the retriever.\n",
    "human_template = '''<|user|>\n",
    "Context: {context}\n",
    "\n",
    "Question is below:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "<|end|>\n",
    "<|assistant|>\n",
    "'''\n",
    "\n",
    "\n",
    "assistant_template = '''<|assistant|>\n",
    "{message}\n",
    "<|end|>\n",
    "'''\n",
    "\n",
    "# Define the templates\n",
    "SystemMessageTemplate = SystemMessagePromptTemplate.from_template(template)\n",
    "HumanMessageTemplate = HumanMessagePromptTemplate.from_template(human_template)\n",
    "AIMessageTemplate = AIMessagePromptTemplate.from_template(assistant_template)\n"
   ],
   "id": "18c906d0107239a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "aa7dc3d79129b2d7",
    "ExecuteTime": {
     "end_time": "2025-03-24T10:50:51.504971Z",
     "start_time": "2025-03-24T10:50:51.460937Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "11f8ec54-eec2-46e5-9a49-06cfb560a37c"
   },
   "cell_type": "code",
   "source": [
    "# Define the system message\n",
    "\n",
    "system_message = '''You are an expert assistant that answers questions about different topics.\n",
    "\n",
    "You are given some extracted parts from science papers along with a question.\n",
    "\n",
    "If you don't know the answer, just say \"I don't know.\" Don't try to make up an answer.\n",
    "\n",
    "Use only the following pieces of context to answer the question at the end.\n",
    "\n",
    "Do not use any prior knowledge.'''\n",
    "\n",
    "\n",
    "system_msg = SystemMessageTemplate.format(message=system_message)\n",
    "\n",
    "system_msg"
   ],
   "id": "aa7dc3d79129b2d7",
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SystemMessage(content='<|system|>\\nYou are an expert assistant that answers questions about different topics.\\n\\nYou are given some extracted parts from science papers along with a question.\\n\\nIf you don\\'t know the answer, just say \"I don\\'t know.\" Don\\'t try to make up an answer.\\n\\nUse only the following pieces of context to answer the question at the end.\\n\\nDo not use any prior knowledge.\\n<|end|>\\n', additional_kwargs={}, response_metadata={})"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "f2a969efb8649e95"
   },
   "cell_type": "markdown",
   "source": [
    "Now that we have the definition of different templates we can define the chat prompt. Langchain requireres a specific structure for the chat prompt that is composed of a list of messages. In the code below we can see that our chat template will be composed of two messages, the **system message** and the **human message** that contains the input from the user."
   ],
   "id": "f2a969efb8649e95"
  },
  {
   "metadata": {
    "id": "1f257d579cd01fba",
    "ExecuteTime": {
     "end_time": "2025-03-24T10:50:55.851538Z",
     "start_time": "2025-03-24T10:50:55.848936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts import MessagesPlaceholder, PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "    system_msg,\n",
    "    human_template,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# As we can see, our prompt is expecting two variables to be filled\n",
    "print(chat_template)"
   ],
   "id": "1f257d579cd01fba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "d527687982594147"
   },
   "cell_type": "markdown",
   "source": [
    "### Model initialization\n"
   ],
   "id": "d527687982594147"
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_aws import BedrockLLM\n",
    "\n",
    "llm = BedrockLLM(model_id='arn:aws:bedrock:us-west-2:637423382292:imported-model/7i06g1utels3', region_name='us-west-2', provider='meta')"
   ],
   "metadata": {
    "id": "bB6Dj3T0usn7"
   },
   "id": "bB6Dj3T0usn7",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Langchain pipelines\n",
    "\n",
    "Langchain pipelines are a powerful tool used to assemble and coordinated different components. Our pipeline will look something like this\n",
    "\n",
    "$$\\text{user query} → \\text{retriever} → \\text{chat prompt} → \\text{LLM} → \\text{answer} $$\n",
    "\n",
    "In langchain we will use the chain '|' operator to assemble in series our components. The chain operator is part of the **LangChain Expression Language** a declarative method to build pipelines. In the LCEL language the output of what is on the left of '|' will be the input on what there is on the right of the pipeline."
   ],
   "metadata": {
    "id": "WwQEuJjSylD_"
   },
   "id": "WwQEuJjSylD_"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's build our first pipeline to understand how they works. In our sample pipeline below, we can see that we are dynamically creating a dictionary that will be given in input to our chat template (N.B. as we saw above our chat template takes in input three variables)\n",
    "\n",
    "From the code we can see that the context value is created by taking the question (from the input dict given to the chain) and using it as input to our retriever. The output of the retriever will be then formatted by the format_docs function.\n",
    "The question instead will remain as it is.\n",
    "\n",
    "\n",
    "A chain will be called by the **invoke** method. The invoke methods takes as argument a dictionary that will represent the input of the first element of the pipeline.\n",
    "\n"
   ],
   "metadata": {
    "id": "QAyEy0c61-YF"
   },
   "id": "QAyEy0c61-YF"
  },
  {
   "metadata": {
    "id": "3519d02e6888bed"
   },
   "cell_type": "code",
   "source": [
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "\n",
    "# Build the pipeline\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"question\": itemgetter('question'),\n",
    "        \"context\": itemgetter('question') | RunnableLambda(retriever.get_relevant_documents) | format_docs,\n",
    "    }\n",
    "    | RunnableLambda(lambda inputs: {\n",
    "        **inputs,\n",
    "        \"prompt\": chat_template.invoke(inputs)  # Add the rendered prompt explicitly\n",
    "    })\n",
    "    | {\n",
    "        \"model_out\": itemgetter(\"prompt\") | llm,\n",
    "        \"prompt\": itemgetter(\"prompt\"),\n",
    "    }\n",
    ")\n",
    "\n",
    "output = rag_chain_from_docs.invoke({\"question\": question})"
   ],
   "id": "3519d02e6888bed",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Print the prompt\n",
    "print(output['prompt'].to_string())\n",
    "print()\n",
    "# Print the model output\n",
    "print(output['model_out'])"
   ],
   "metadata": {
    "id": "IKadiCuu4drC"
   },
   "id": "IKadiCuu4drC",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
