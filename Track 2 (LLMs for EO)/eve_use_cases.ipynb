{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Use Case Demonstration of EVE\n",
    "In this notebook, we explore practical applications of EVE (Earth Virtual Explorer), a large language model specialized in Earth Observation (EO). EVE is designed to understand, analyze, and generate text related to EO data and topics, making it a valuable tool for researchers, analysts, and decision-makers in the field.\n",
    "\n",
    "We will demonstrate two key use cases of EVE:\n",
    "\n",
    "- **Summarization**: In this task, EVE is given a document related to Earth Observation and asked to generate a concise and informative summary. This is useful for quickly understanding lengthy reports, scientific papers, or satellite data documentation.\n",
    "\n",
    "- **Question Answering (Q&A)**: In this use case, we enhance EVE’s performance by integrating it with a retrieval system, using a technique known as Retrieval-Augmented Generation (RAG). Here, the model first retrieves relevant context from a knowledge base before answering questions, leading to more accurate and grounded responses.\n",
    "\n",
    "These examples highlight EVE’s potential to streamline information processing and support decision-making in Earth Observation workflows."
   ],
   "metadata": {
    "id": "6rpB4jm2WujE"
   },
   "id": "6rpB4jm2WujE"
  },
  {
   "cell_type": "code",
   "source": [
    "# Install the required libraries\n",
    "!pip3 install -q -U bitsandbytes\n",
    "!pip3 install -q datasets\n",
    "!pip3 install -q langchain_community\n",
    "!pip3 install -q pypdf\n",
    "!pip3 install -q sentence-transformers\n",
    "!pip3 install -q faiss-cpu\n",
    "!pip3 install -q langchain_huggingface\n",
    "!pip3 install -q langchain_runpod\n",
    "!pip3 install -q qdrant_client\n",
    "!pip3 install -q langchain_aws\n",
    "!pip3 install numpy==1.26.4\n"
   ],
   "metadata": {
    "id": "pZd6NRpIZIJ-"
   },
   "id": "pZd6NRpIZIJ-",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summarization"
   ],
   "metadata": {
    "id": "udcAFT-wWzlc"
   },
   "id": "udcAFT-wWzlc"
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import  load_dataset\n",
    "import random\n",
    "# Load documents\n",
    "docs = load_dataset('eve-esa/eve-cpt-sample-v0.2')['train']\n",
    "\n",
    "# Select a random doc from the dataset\n",
    "doc = docs.select(random.sample(range(len(docs)), 1))[0]"
   ],
   "metadata": {
    "id": "ZkmZqeqPWuEF"
   },
   "id": "ZkmZqeqPWuEF",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_aws import BedrockLLM\n",
    "\n",
    "# Load the model\n",
    "llm = BedrockLLM(model_id='arn:aws:bedrock:us-west-2:637423382292:imported-model/7i06g1utels3', region_name='us-west-2', provider='meta')"
   ],
   "metadata": {
    "id": "Nu1eHm5tXoq_"
   },
   "id": "Nu1eHm5tXoq_",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Promp message\n",
    "\n",
    "message = f\"\"\"\n",
    "<|system|>\n",
    "You are an helpful assistant expert in Earth Observation, help the user with his tasks.\n",
    "<|end|>\n",
    "<|user|>\n",
    "Summarize the following document focusing on the main concepts and ideas.\n",
    "\n",
    "The document starts\n",
    "{doc['text']}\n",
    "\n",
    "<|end|>\n",
    "<|assistant|>\n",
    "\"\"\"\n"
   ],
   "metadata": {
    "id": "uSYFscIPfgSM"
   },
   "id": "uSYFscIPfgSM",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Run this cell to show the final prompt given to the model\n",
    "print(message)"
   ],
   "metadata": {
    "id": "wI7y0uBl0nDP"
   },
   "id": "wI7y0uBl0nDP",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "output = llm.invoke(message)"
   ],
   "metadata": {
    "id": "tKyDu4JKedrX"
   },
   "id": "tKyDu4JKedrX",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "output"
   ],
   "metadata": {
    "id": "6B6G3af6ciOM"
   },
   "id": "6B6G3af6ciOM",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "6c07c15e489c973a"
   },
   "cell_type": "markdown",
   "source": [
    "# Retrieval Augmented Generation (RAG) with Langchain\n",
    "\n",
    "In this notebook, we implement a complete RAG pipeline for answering questions based on a given context. Using the LangChain library, we'll walk through the entire process—from retrieving relevant context to generating accurate answers.\n"
   ],
   "id": "6c07c15e489c973a"
  },
  {
   "metadata": {
    "id": "b793ab84cc858c7f"
   },
   "cell_type": "markdown",
   "source": [
    "**Roadmap**\n",
    "\n",
    "1. **Indexing**: Organize the raw documents into a structured format suitable for processing, such as splitting them into chunks or passages for more efficient retrieval.\n",
    "\n",
    "2. **Embedding**: Convert each text chunk into a dense vector representation using a pre-trained embedding model. These embeddings capture the semantic meaning of the content.\n",
    "\n",
    "3. **Vector Store**: Store the embeddings in a vector database (Qdrant in our case), allowing fast and scalable similarity search across the document collection.\n",
    "\n",
    "4. **Retrieval and Generation**: Given a user query, retrieve the most relevant document chunks from the vector store and feed them into a language model (EVE) to generate a context-aware, accurate response."
   ],
   "id": "b793ab84cc858c7f"
  },
  {
   "metadata": {
    "id": "6f19179e8af8ce6a"
   },
   "cell_type": "markdown",
   "source": [
    "## Load dataset of Q&A\n",
    "Let's load our dataset of Q&A about EO, each sample is composed of a question and an answer"
   ],
   "id": "6f19179e8af8ce6a"
  },
  {
   "metadata": {
    "id": "b733b528b619214"
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "qa = load_dataset('eve-esa/eve-is-open-ended')['train']\n",
    "\n",
    "idx = 120\n",
    "\n",
    "question = qa[idx]['question']\n",
    "answer = qa[idx]['answer']\n",
    "\n",
    "\n",
    "print('Question: ', question)\n",
    "print('Answer: ', answer)"
   ],
   "id": "b733b528b619214",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "1fe9e4d1eed5cb3"
   },
   "cell_type": "markdown",
   "source": [
    "## Indexing\n",
    "\n",
    "The first part of a RAG pipeline is called **indexing**. This is the process of ingesting data from a source and indexing it. The indexing process is composed of three steps:\n",
    "- **Load**: process and load data in text format.\n",
    "- **Split**: this is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window.\n",
    "- **Store**: we need somewhere to store and index our splits, so that they can be searched over later. This is often done using a [VectorStore](https://python.langchain.com/docs/concepts/vectorstores/) and [Embeddings](https://python.langchain.com/docs/concepts/embedding_models/) model.\n",
    "Once the Indexing step is done we will have our knowledge base made of scientific papers indexed and ready to be used in the generation steps as context.\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"https://python.langchain.com/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png\" width=\"800\"/>\n",
    "</div>\n"
   ],
   "id": "1fe9e4d1eed5cb3"
  },
  {
   "metadata": {
    "id": "1e6fe2ba335ad8fe"
   },
   "cell_type": "markdown",
   "source": [
    "### Embeddings\n",
    "\n",
    "An embeddings model in Retrieval-Augmented Generation (RAG) is a neural network that converts text into dense vector representations (embeddings) in a **high-dimensional space**. These models take text as input and produce a fixed-length array of numbers, a numerical fingerprint of the text's semantic meaning. Embeddings allow search system to find relevant documents not just based on keyword matches, but on semantic understanding.\n",
    "\n",
    "Embeddings models are trained on large text corpora using unsupervised learning techniques. They learn to encode the semantic meaning of words, sentences, and documents in a way that captures relationships between them. For example, embeddings models can learn that \"cat\" and \"dog\" are similar because they are both animals, or that \"apple\" and \"orange\" are similar because they are both fruits.\n",
    "\n",
    "There are many pre-trained embedding models available, each suited to different types of data and use cases. For our application, we use Indus, a fine-tuned encoder-only transformer model trained specifically on scientific journals and articles related to NASA’s Science Mission Directorate (SMD).\n",
    "\n",
    "Choosing the right embedding model is a critical step in building an effective retrieval system. Ideally, the embedding model should be trained—or at least fine-tuned—on data similar to the target documents. Since our corpus consists of scientific texts focused on Earth Observation, Indus is a better fit than a general-purpose model, as it captures domain-specific terminology and semantics more accurately.\n",
    "\n",
    "<img src=\"https://weaviate.io/assets/images/embedding-models-0c04d93c0be28dd63a0e8781c4e8685d.jpg\" width='800px'>\n",
    "\n",
    "\n"
   ],
   "id": "1e6fe2ba335ad8fe"
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load the embeddings model\n",
    "model_name = \"nasa-impact/nasa-smd-ibm-st-v2\"\n",
    "encode_kwargs = {\"normalize_embeddings\": True}\n",
    "indus_embd = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,  encode_kwargs=encode_kwargs\n",
    ")"
   ],
   "metadata": {
    "id": "XYz4pvs3jD7S"
   },
   "id": "XYz4pvs3jD7S",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "8746b6b79e3c4d83"
   },
   "cell_type": "markdown",
   "source": [
    "### Vector Store\n",
    "\n",
    "Vector stores are specialized databases designed to efficiently index and retrieve information using vector representations of data. Vector stores leverages the dense representation by reducing the task of finding similar documents to a search in a high-dimensional space. This search is made by comparing the vector representation of the **query** with the vector representation of the **documents** in the database. The documents that are closer to the query vector are considered more similar to the query.\n",
    "\n",
    "Wrapping up the retrieval process is composed of:\n",
    "- **Documents embedding**\n",
    "- **Store the embeddings in a VectorStore**\n",
    "- **Query embedding**\n",
    "- **Retrieve** the most similar documents to the query\n",
    "\n",
    "\n",
    "The most popular and simple setup is using the **cosine similarity** to compare the vectors and retrieve the **top k** most similar ones\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"https://python.langchain.com/assets/images/vectorstores-2540b4bc355b966c99b0f02cfdddb273.png\" width=\"800\"/>\n",
    "</div>\n"
   ],
   "id": "8746b6b79e3c4d83"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Connect to QDrant\n",
    "\n",
    "To save time, the embedding and indexing of documents have already been completed prior to this notebook. These steps can be computationally intensive, so we’ve pre-processed the data to streamline the workflow.\n",
    "\n",
    "We are using Qdrant as our vector store, which has been preloaded with all the relevant documents needed for retrieval. In this section, we will connect to the Qdrant instance and select the specific collection that contains our indexed data. This will enable us to perform efficient semantic searches and support the Retrieval-Augmented Generation (RAG) process used in our Q&A tasks."
   ],
   "metadata": {
    "id": "HWgZw1keeYtq"
   },
   "id": "HWgZw1keeYtq"
  },
  {
   "cell_type": "code",
   "source": [
    "# Examples of retrieval pipeline using the embedding function and the API from QDrant\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "qdrant_url = 'https://e186510c-4dd9-45c7-99a5-ae38c4c8bc36.us-east-1-0.aws.cloud.qdrant.io:6333'\n",
    "api_key = 'rZYblMkzsiqiiuPqxXxmckfyMFIZ9Yg9EpxYxhbeFZj82MEOIbT5Fg'\n",
    "\n",
    "# Enstablish a connection wit the vector store\n",
    "client = QdrantClient(\n",
    "    url=qdrant_url,\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# Embedd the query\n",
    "query_emb = indus_embd.embed_query(question)\n",
    "\n",
    "# Perform similarity search using the computed embeddings\n",
    "search_result = client.search(\n",
    "    collection_name=\"indus-test\",\n",
    "    query_vector=query_emb,\n",
    "    limit=1,\n",
    ")\n",
    "\n",
    "data = search_result[0].payload\n",
    "# Payload containing metadata and text\n",
    "for key, value in data['metadata'].items():\n",
    "  print(f'{key}: {value}')\n",
    "\n",
    "print('Retrieved chunk:\\n', data['page_content'])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "OBzZdrljQXHv",
    "outputId": "b150051b-beb0-4a6f-b7f7-cdbf5b60b834"
   },
   "id": "OBzZdrljQXHv",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from typing import List\n",
    "from qdrant_client import QdrantClient\n",
    "from pydantic import PrivateAttr, Field\n",
    "from typing import List, Optional, Dict\n",
    "from qdrant_client.models import Filter, PointStruct\n",
    "\n",
    "from langchain_core.callbacks import CallbackManagerForRetrieverRun\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Let's define our retriever class to have a nice interface\n",
    "class RunpodRetriever():\n",
    "    def __init__(self, embedding, collection_name='indus-test', k: int = 3):\n",
    "        self._client = QdrantClient(url=\"https://e186510c-4dd9-45c7-99a5-ae38c4c8bc36.us-east-1-0.aws.cloud.qdrant.io:6333\",\n",
    "        api_key=\"rZYblMkzsiqiiuPqxXxmckfyMFIZ9Yg9EpxYxhbeFZj82MEOIbT5Fg\")\n",
    "        self.embedding = embedding\n",
    "        self.collection_name = collection_name\n",
    "        self.k = k\n",
    "\n",
    "\n",
    "    def get_relevant_documents(self, query: str) -> List[Document]:\n",
    "        query_emb = self.embedding.embed_query(query)\n",
    "\n",
    "        search_result = self._client.search(\n",
    "            collection_name=self.collection_name,\n",
    "            query_vector=query_emb,\n",
    "            limit=self.k,\n",
    "        )\n",
    "\n",
    "        docs = []\n",
    "        for hit in search_result:\n",
    "            # Adjust based on your actual data structure\n",
    "            data = hit.payload\n",
    "            content = data.get(\"page_content\", \"\")\n",
    "            metadata = data.get(\"metadata\", {})\n",
    "            docs.append(Document(page_content=content, metadata=metadata))\n",
    "\n",
    "        return docs\n"
   ],
   "metadata": {
    "id": "3ZqrzBMFbcw9"
   },
   "id": "3ZqrzBMFbcw9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Let's define our retriever\n",
    "retriever = RunpodRetriever(indus_embd, k=3)\n",
    "\n",
    "# Format retrieved documents:\n",
    "def format_docs(docs):\n",
    "  doc_str = ''\n",
    "  for i, doc in enumerate(docs):\n",
    "    doc_str += f'Document n. {i+1}\\n'\n",
    "    doc_str += f'TITLE: {doc.metadata.get(\"source_name\", \"No title\")}\\n' # Add title's of the paper\n",
    "    doc_str += f'URL: {doc.metadata.get(\"source\", \"No url\")}\\n\\n' # Add URL of the paper\n",
    "    doc_str += f'{doc.page_content}\\n\\n'\n",
    "  return doc_str\n",
    "\n",
    "\n",
    "\n",
    "print('Question: ')\n",
    "print(question)\n",
    "print()\n",
    "docs = retriever.get_relevant_documents(question)\n",
    "print(format_docs(docs))"
   ],
   "metadata": {
    "id": "ApDagHKExlZa"
   },
   "id": "ApDagHKExlZa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "b019ad14ade15fcf"
   },
   "cell_type": "markdown",
   "source": [
    "## Retrieval and generation\n",
    "\n",
    "The pipeline consists of the following key components:\n",
    "\n",
    "- Retriever: This component queries the vector store (Qdrant) to fetch the most relevant document chunks based on the user’s question. It performs a semantic search using the pre-computed embeddings to find contextually similar content.\n",
    "\n",
    "- LLM (Large Language Model): Once the relevant context is retrieved, it is passed to EVE, our Earth Observation-specialized language model. EVE then generates a coherent and informed response based on both the query and the retrieved context.\n",
    "\n",
    "This approach ensures that the generated answers are grounded in the source documents, improving accuracy and reducing hallucination."
   ],
   "id": "b019ad14ade15fcf"
  },
  {
   "metadata": {
    "id": "a2e01ecdefdfa5a6"
   },
   "cell_type": "markdown",
   "source": [
    "### Prompt\n",
    "\n",
    "First, we will define the prompt to be used  using three different templates:\n",
    "- **SystemMessagePromptTemplate**: the system message represents guidelines for the model on how to interact with the user and interpret the conversation.\n",
    "- **AIMessagePromptTemplate**: the AI message represents a message generate by the model.\n",
    "- **HumanMessagePromptTemplate**: the human message represents the message sent by the user.\n",
    "\n",
    "From the code below we can see the structure of the prompt and the templates used to create it using langchain. Specifically we could see some **special tokens** used in the prompt:\n",
    "- **<|system|> | <|user|> | <|assistant|>**: are special tokens that helps the model to understand to who belongs that specific message.\n",
    "- **<|end|>**: is a special token that indicates the end of a message.\n",
    "- **{message} | {context} | {question}**: are placeholders that will be replaced with the actual message, context and question."
   ],
   "id": "a2e01ecdefdfa5a6"
  },
  {
   "metadata": {
    "id": "18c906d0107239a5"
   },
   "cell_type": "code",
   "source": [
    "import os  # Customize SystemPromptTemplate\n",
    "from langchain.prompts import SystemMessagePromptTemplate, AIMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "template= '''<|system|>\n",
    "{message}\n",
    "<|end|>\n",
    "'''\n",
    "\n",
    "# A human message will contain the question and the context. The context will be automatically added by the retriever.\n",
    "human_template = '''<|user|>\n",
    "Context: {context}\n",
    "\n",
    "Question is below:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "<|end|>\n",
    "<|assistant|>\n",
    "'''\n",
    "\n",
    "\n",
    "assistant_template = '''<|assistant|>\n",
    "{message}\n",
    "<|end|>\n",
    "'''\n",
    "\n",
    "# Define the templates\n",
    "SystemMessageTemplate = SystemMessagePromptTemplate.from_template(template)\n",
    "HumanMessageTemplate = HumanMessagePromptTemplate.from_template(human_template)\n",
    "AIMessageTemplate = AIMessagePromptTemplate.from_template(assistant_template)\n"
   ],
   "id": "18c906d0107239a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "aa7dc3d79129b2d7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "11f8ec54-eec2-46e5-9a49-06cfb560a37c"
   },
   "cell_type": "code",
   "source": [
    "# Define the system message\n",
    "\n",
    "system_message = '''You are an expert assistant that answers questions about different topics.\n",
    "\n",
    "You are given some extracted parts from science papers along with a question.\n",
    "\n",
    "If you don't know the answer, just say \"I don't know.\" Don't try to make up an answer.\n",
    "\n",
    "Use only the following pieces of context to answer the question at the end.\n",
    "\n",
    "Do not use any prior knowledge.'''\n",
    "\n",
    "\n",
    "system_msg = SystemMessageTemplate.format(message=system_message)\n",
    "\n",
    "system_msg"
   ],
   "id": "aa7dc3d79129b2d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "f2a969efb8649e95"
   },
   "cell_type": "markdown",
   "source": [
    "Now that we have the definition of different templates we can define the chat prompt. Langchain requireres a specific structure for the chat prompt that is composed of a list of messages. In the code below we can see that our chat template will be composed of two messages, the **system message** and the **human message** that contains the input from the user."
   ],
   "id": "f2a969efb8649e95"
  },
  {
   "metadata": {
    "id": "1f257d579cd01fba",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "54f5a510-7c84-4428-8cbe-a4af5fe82655"
   },
   "cell_type": "code",
   "source": [
    "from langchain.prompts import MessagesPlaceholder, PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    messages=[\n",
    "    system_msg,\n",
    "    human_template,\n",
    "    ]\n",
    ")\n",
    "\n",
    "# As we can see, our prompt is expecting two variables to be filled\n",
    "print(chat_template)"
   ],
   "id": "1f257d579cd01fba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "d527687982594147"
   },
   "cell_type": "markdown",
   "source": [
    "### Model initialization\n"
   ],
   "id": "d527687982594147"
  },
  {
   "cell_type": "code",
   "source": [
    "from langchain_aws import BedrockLLM\n",
    "\n",
    "llm = BedrockLLM(model_id='arn:aws:bedrock:us-west-2:637423382292:imported-model/7i06g1utels3', region_name='us-west-2', provider='meta')"
   ],
   "metadata": {
    "id": "bB6Dj3T0usn7"
   },
   "id": "bB6Dj3T0usn7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Langchain pipelines\n",
    "\n",
    "Langchain pipelines are a powerful tool used to assemble and coordinated different components. Our pipeline will look something like this\n",
    "\n",
    "$$\\text{user query} → \\text{retriever} → \\text{chat prompt} → \\text{LLM} → \\text{answer} $$\n",
    "\n",
    "In langchain we will use the chain '|' operator to assemble in series our components. The chain operator is part of the **LangChain Expression Language** a declarative method to build pipelines. In the LCEL language the output of what is on the left of '|' will be the input on what there is on the right of the pipeline."
   ],
   "metadata": {
    "id": "WwQEuJjSylD_"
   },
   "id": "WwQEuJjSylD_"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's build our first pipeline to understand how they works. In our sample pipeline below, we can see that we are dynamically creating a dictionary that will be given in input to our chat template (N.B. as we saw above our chat template takes in input three variables)\n",
    "\n",
    "From the code we can see that the context value is created by taking the question (from the input dict given to the chain) and using it as input to our retriever. The output of the retriever will be then formatted by the format_docs function.\n",
    "The question instead will remain as it is.\n",
    "\n",
    "\n",
    "A chain will be called by the **invoke** method. The invoke methods takes as argument a dictionary that will represent the input of the first element of the pipeline.\n",
    "\n"
   ],
   "metadata": {
    "id": "QAyEy0c61-YF"
   },
   "id": "QAyEy0c61-YF"
  },
  {
   "metadata": {
    "id": "3519d02e6888bed"
   },
   "cell_type": "code",
   "source": [
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "\n",
    "# Build the pipeline\n",
    "rag_chain_from_docs = (\n",
    "    {\n",
    "        \"question\": itemgetter('question'),\n",
    "        \"context\": itemgetter('question') | RunnableLambda(retriever.get_relevant_documents) | format_docs,\n",
    "    }\n",
    "    | RunnableLambda(lambda inputs: {\n",
    "        **inputs,\n",
    "        \"prompt\": chat_template.invoke(inputs)  # Add the rendered prompt explicitly\n",
    "    })\n",
    "    | {\n",
    "        \"model_out\": itemgetter(\"prompt\") | llm,\n",
    "        \"prompt\": itemgetter(\"prompt\"),\n",
    "    }\n",
    ")\n",
    "\n",
    "output = rag_chain_from_docs.invoke({\"question\": question})"
   ],
   "id": "3519d02e6888bed",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Print the prompt\n",
    "print(output['prompt'].to_string())\n",
    "print()\n",
    "# Print the model output\n",
    "print(output['model_out'])"
   ],
   "metadata": {
    "id": "IKadiCuu4drC"
   },
   "id": "IKadiCuu4drC",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
