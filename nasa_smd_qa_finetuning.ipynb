{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# NASA SMD Question Answering Finetuning\n",
                "\n",
                "This notebook demonstrates how to finetune a transformer model for extractive question answering using the NASA SMD QA benchmark dataset. We'll use the Hugging Face transformers library to fine-tune a pretrained language model on this task."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Install Required Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install transformers datasets accelerate evaluate scikit-learn matplotlib"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import torch\n",
                "from datasets import load_dataset\n",
                "from transformers import (\n",
                "    AutoTokenizer,\n",
                "    AutoModelForQuestionAnswering,\n",
                "    TrainingArguments,\n",
                "    Trainer,\n",
                "    default_data_collator,\n",
                "    EvalPrediction\n",
                ")\n",
                "import evaluate\n",
                "import collections\n",
                "import matplotlib.pyplot as plt\n",
                "from tqdm.auto import tqdm"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Check for GPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load the NASA SMD QA Benchmark Dataset\n",
                "\n",
                "This dataset is in SQuAD v2 format, which includes questions with and without answers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the dataset\n",
                "dataset = load_dataset(\"nasa-impact/nasa-smd-qa-benchmark\")\n",
                "print(f\"Dataset loaded: {dataset}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Explore the dataset structure\n",
                "print(\"Dataset structure:\")\n",
                "print(dataset[\"train\"][0])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Initialize the Model and Tokenizer\n",
                "\n",
                "We'll use a pretrained model as our starting point for finetuning."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize the tokenizer and model\n",
                "model_name = \"distilbert-base-uncased\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
                "\n",
                "print(f\"Model and tokenizer initialized from {model_name}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preprocess the Dataset\n",
                "\n",
                "For question answering tasks, we need to:\n",
                "1. Tokenize both the question and context\n",
                "2. Find start and end positions of answers in the tokenized context\n",
                "3. Handle no-answer cases (SQuAD v2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Maximum sequence length for the model\n",
                "max_length = 384\n",
                "# Maximum length of the answer that can be generated\n",
                "max_answer_length = 30\n",
                "# Stride to use when splitting up a long context\n",
                "doc_stride = 128\n",
                "# Set to True if we have unanswerable questions (SQuAD v2)\n",
                "squad_v2 = True\n",
                "\n",
                "# Function to prepare the dataset features\n",
                "def prepare_train_features(examples):\n",
                "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
                "    # truncation of the context fail (the tokenized question will take a lots of space).\n",
                "    # So we remove that left whitespace\n",
                "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
                "\n",
                "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride.\n",
                "    # This results in one example possibly giving several features when a context is long.\n",
                "    tokenized_examples = tokenizer(\n",
                "        examples[\"question\"],\n",
                "        examples[\"context\"],\n",
                "        truncation=\"only_second\",\n",
                "        max_length=max_length,\n",
                "        stride=doc_stride,\n",
                "        return_overflowing_tokens=True,\n",
                "        return_offsets_mapping=True,\n",
                "        padding=\"max_length\",\n",
                "    )\n",
                "\n",
                "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
                "    # its corresponding example. This key gives us that.\n",
                "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
                "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
                "    # help us compute the start_positions and end_positions.\n",
                "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
                "\n",
                "    # Let's label those examples!\n",
                "    tokenized_examples[\"start_positions\"] = []\n",
                "    tokenized_examples[\"end_positions\"] = []\n",
                "\n",
                "    for i, offsets in enumerate(offset_mapping):\n",
                "        # We will label impossible answers with the index of the CLS token.\n",
                "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
                "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
                "\n",
                "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
                "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
                "\n",
                "        # One example can give several spans, this is the index of the example containing this span of text.\n",
                "        sample_index = sample_mapping[i]\n",
                "        answers = examples[\"answers\"][sample_index]\n",
                "        # If no answers are given, set the cls_index as answer.\n",
                "        if len(answers[\"answer_start\"]) == 0:\n",
                "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
                "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
                "        else:\n",
                "            # Start/end character index of the answer in the text.\n",
                "            start_char = answers[\"answer_start\"][0]\n",
                "            end_char = start_char + len(answers[\"text\"][0])\n",
                "\n",
                "            # Start token index of the current span in the text.\n",
                "            token_start_index = 0\n",
                "            while sequence_ids[token_start_index] != 1:\n",
                "                token_start_index += 1\n",
                "\n",
                "            # End token index of the current span in the text.\n",
                "            token_end_index = len(input_ids) - 1\n",
                "            while sequence_ids[token_end_index] != 1:\n",
                "                token_end_index -= 1\n",
                "\n",
                "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
                "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
                "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
                "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
                "            else:\n",
                "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
                "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
                "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
                "                    token_start_index += 1\n",
                "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
                "                while offsets[token_end_index][1] >= end_char:\n",
                "                    token_end_index -= 1\n",
                "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
                "\n",
                "    return tokenized_examples\n",
                "\n",
                "# Similar function for validation data, but with additional answer processing\n",
                "def prepare_validation_features(examples):\n",
                "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
                "\n",
                "    tokenized_examples = tokenizer(\n",
                "        examples[\"question\"],\n",
                "        examples[\"context\"],\n",
                "        truncation=\"only_second\",\n",
                "        max_length=max_length,\n",
                "        stride=doc_stride,\n",
                "        return_overflowing_tokens=True,\n",
                "        return_offsets_mapping=True,\n",
                "        padding=\"max_length\",\n",
                "    )\n",
                "\n",
                "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
                "    # its corresponding example. This key gives us just that.\n",
                "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
                "\n",
                "    # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n",
                "    # corresponding example_id and we will store the offset mappings.\n",
                "    tokenized_examples[\"example_id\"] = []\n",
                "\n",
                "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
                "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
                "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
                "        context_index = 1\n",
                "\n",
                "        # One example can give several spans, this is the index of the example containing this span of text.\n",
                "        sample_index = sample_mapping[i]\n",
                "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
                "\n",
                "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
                "        # position is part of the context or not.\n",
                "        tokenized_examples[\"offset_mapping\"][i] = [\n",
                "            (o if sequence_ids[k] == context_index else None)\n",
                "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
                "        ]\n",
                "\n",
                "    return tokenized_examples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply the preprocessing functions\n",
                "tokenized_train_dataset = dataset[\"train\"].map(\n",
                "    prepare_train_features,\n",
                "    batched=True,\n",
                "    remove_columns=dataset[\"train\"].column_names,\n",
                ")\n",
                "\n",
                "tokenized_validation_dataset = dataset[\"validation\"].map(\n",
                "    prepare_validation_features,\n",
                "    batched=True,\n",
                "    remove_columns=dataset[\"validation\"].column_names,\n",
                ")\n",
                "\n",
                "print(f\"Tokenized train dataset: {tokenized_train_dataset}\")\n",
                "print(f\"Tokenized validation dataset: {tokenized_validation_dataset}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluation Metrics\n",
                "\n",
                "For question answering, we need to compute the exact match and F1 score between the predicted answers and the reference answers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# The metric to use for evaluation\n",
                "metric = evaluate.load(\"squad_v2\" if squad_v2 else \"squad\")\n",
                "\n",
                "# Function to post process the results and compute metrics\n",
                "def post_processing_function(examples, features, predictions, stage=\"eval\"):\n",
                "    # Post-processing: we match the start logits and end logits to answers in the original context.\n",
                "    predictions = postprocess_qa_predictions(\n",
                "        examples=examples,\n",
                "        features=features,\n",
                "        predictions=predictions,\n",
                "        version_2_with_negative=squad_v2,\n",
                "        n_best_size=20,\n",
                "        max_answer_length=max_answer_length,\n",
                "        null_score_diff_threshold=0.0,\n",
                "        output_dir=None,\n",
                "        prefix=stage,\n",
                "    )\n",
                "    \n",
                "    # Format the result to the format the metric expects.\n",
                "    if squad_v2:\n",
                "        formatted_predictions = [\n",
                "            {\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()\n",
                "        ]\n",
                "    else:\n",
                "        formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n",
                "\n",
                "    references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples]\n",
                "    return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n",
                "\n",
                "def compute_metrics(p: EvalPrediction):\n",
                "    return metric.compute(predictions=p.predictions, references=p.label_ids)\n",
                "\n",
                "# Post-processing function for question answering predictions\n",
                "def postprocess_qa_predictions(\n",
                "    examples,\n",
                "    features,\n",
                "    predictions,\n",
                "    version_2_with_negative=False,\n",
                "    n_best_size=20,\n",
                "    max_answer_length=30,\n",
                "    null_score_diff_threshold=0.0,\n",
                "    output_dir=None,\n",
                "    prefix=None,\n",
                "):\n",
                "    if len(predictions) != 2:\n",
                "        raise ValueError(\"predictions should be of size 2, but is {}\".format(len(predictions)))\n",
                "    all_start_logits, all_end_logits = predictions\n",
                "\n",
                "    # Build a map from example to its features\n",
                "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
                "    features_per_example = collections.defaultdict(list)\n",
                "    for i, feature in enumerate(features):\n",
                "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
                "\n",
                "    # The dictionaries we have to fill\n",
                "    all_predictions = collections.OrderedDict()\n",
                "\n",
                "    # Let's loop over all the examples\n",
                "    for example_index, example in enumerate(tqdm(examples)):\n",
                "        # Those are the indices of the features associated to the current example\n",
                "        feature_indices = features_per_example[example_index]\n",
                "\n",
                "        min_null_prediction = None\n",
                "        prelim_predictions = []\n",
                "\n",
                "        # Looping through all the features associated to the current example\n",
                "        for feature_index in feature_indices:\n",
                "            # We need to get the token mappings from the indices to the text\n",
                "            current_feature = features[feature_index]\n",
                "            offset_mapping = current_feature[\"offset_mapping\"]\n",
                "\n",
                "            # First feature has the CLS index\n",
                "            cls_index = current_feature[\"input_ids\"].index(tokenizer.cls_token_id)\n",
                "\n",
                "            # Get the prediction for start and end positions\n",
                "            start_logits = all_start_logits[feature_index]\n",
                "            end_logits = all_end_logits[feature_index]\n",
                "\n",
                "            # Get the score for the \"no answer\" option\n",
                "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
                "            if min_null_prediction is None or min_null_prediction[\"score\"] > feature_null_score:\n",
                "                min_null_prediction = {\n",
                "                    \"offsets\": (0, 0),\n",
                "                    \"score\": feature_null_score,\n",
                "                    \"start_logit\": start_logits[cls_index],\n",
                "                    \"end_logit\": end_logits[cls_index],\n",
                "                }\n",
                "\n",
                "            # Go through all possible answer spans and score them\n",
                "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
                "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
                "            for start_index in start_indexes:\n",
                "                for end_index in end_indexes:\n",
                "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
                "                    # to part of the input_ids that are not in the context.\n",
                "                    if (\n",
                "                        start_index >= len(offset_mapping)\n",
                "                        or end_index >= len(offset_mapping)\n",
                "                        or offset_mapping[start_index] is None\n",
                "                        or offset_mapping[end_index] is None\n",
                "                    ):\n",
                "                        continue\n",
                "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
                "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
                "                        continue\n",
                "\n",
                "                    prelim_predictions.append(\n",
                "                        {\n",
                "                            \"offsets\": (offset_mapping[start_index][0], offset_mapping[end_index][1]),\n",
                "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
                "                            \"start_logit\": start_logits[start_index],\n",
                "                            \"end_logit\": end_logits[end_index],\n",
                "                        }\n",
                "                    )\n",
                "\n",
                "        # If we don't have any valid predictions, just return the empty string\n",
                "        if len(prelim_predictions) == 0:\n",
                "            all_predictions[example[\"id\"]] = \"\"\n",
                "        else:\n",
                "            # Only keep the best `n_best_size` predictions\n",
                "            predictions = sorted(prelim_predictions, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
                "\n",
                "            # Score differential for \"no answer\" prediction\n",
                "            if version_2_with_negative:\n",
                "                if min_null_prediction is not None:\n",
                "                    # Predict \"no answer\" only if its score is the best by a margin over other answers\n",
                "                    score_diff = min_null_prediction[\"score\"] - predictions[0][\"score\"]\n",
                "                    if score_diff > null_score_diff_threshold:\n",
                "                        all_predictions[example[\"id\"]] = \"\"\n",
                "                    else:\n",
                "                        all_predictions[example[\"id\"]] = example[\"context\"][predictions[0][\"offsets\"][0]:predictions[0][\"offsets\"][1]]\n",
                "                else:\n",
                "                    all_predictions[example[\"id\"]] = example[\"context\"][predictions[0][\"offsets\"][0]:predictions[0][\"offsets\"][1]]\n",
                "            else:\n",
                "                all_predictions[example[\"id\"]] = example[\"context\"][predictions[0][\"offsets\"][0]:predictions[0][\"offsets\"][1]]\n",
                "\n",
                "    return all_predictions"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Configure Training Parameters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set training parameters\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./nasa-smd-qa-model\",\n",
                "    evaluation_strategy=\"epoch\",\n",
                "    learning_rate=3e-5,\n",
                "    per_device_train_batch_size=16,\n",
                "    per_device_eval_batch_size=16,\n",
                "    num_train_epochs=3,\n",
                "    weight_decay=0.01,\n",
                "    push_to_hub=False,\n",
                "    save_strategy=\"epoch\",\n",
                "    save_total_limit=2,\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Initialize Trainer and Start Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize our Trainer\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=tokenized_train_dataset,\n",
                "    eval_dataset=tokenized_validation_dataset,\n",
                "    tokenizer=tokenizer,\n",
                "    data_collator=default_data_collator,\n",
                "    post_process_function=lambda examples, features, predictions: post_processing_function(dataset[\"validation\"], features, predictions),\n",
                "    compute_metrics=compute_metrics,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Start training\n",
                "print(\"Starting training...\")\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluate the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate the model\n",
                "print(\"Evaluating the model...\")\n",
                "result = trainer.evaluate()\n",
                "print(result)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Test Model with Sample Questions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Function to get answer from the model for a given question and context\n",
                "def get_answer(question, context):\n",
                "    inputs = tokenizer(\n",
                "        question,\n",
                "        context,\n",
                "        return_tensors=\"pt\",\n",
                "        max_length=max_length,\n",
                "        truncation=\"only_second\",\n",
                "        padding=\"max_length\",\n",
                "    ).to(device)\n",
                "    \n",
                "    model.to(device)\n",
                "    with torch.no_grad():\n",
                "        outputs = model(**inputs)\n",
                "    \n",
                "    answer_start = torch.argmax(outputs.start_logits)\n",
                "    answer_end = torch.argmax(outputs.end_logits)\n",
                "    \n",
                "    if answer_start >= len(inputs[\"input_ids\"][0]) or answer_end >= len(inputs[\"input_ids\"][0]):\n",
                "        return \"No answer found\"\n",
                "    \n",
                "    # Convert to numpy for easier manipulation\n",
                "    input_ids = inputs[\"input_ids\"][0].cpu().numpy()\n",
                "    \n",
                "    # Check if the answer is a no-answer prediction\n",
                "    if answer_start == 0 and answer_end == 0:\n",
                "        return \"No answer found\"\n",
                "    \n",
                "    answer = tokenizer.decode(input_ids[answer_start:answer_end+1], skip_special_tokens=True)\n",
                "    return answer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test the model with a few examples from the validation set\n",
                "for i in range(5):  # Test 5 examples\n",
                "    example = dataset[\"validation\"][i]\n",
                "    print(f\"\\nExample {i+1}:\")\n",
                "    print(f\"Context: {example['context'][:200]}...\")\n",
                "    print(f\"Question: {example['question']}\")\n",
                "    print(f\"True answer: {example['answers']['text']}\")\n",
                "    predicted_answer = get_answer(example['question'], example['context'])\n",
                "    print(f\"Predicted answer: {predicted_answer}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save the Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the model and tokenizer\n",
                "model_save_path = \"./nasa-smd-qa-final\"\n",
                "model.save_pretrained(model_save_path)\n",
                "tokenizer.save_pretrained(model_save_path)\n",
                "print(f\"Model saved to {model_save_path}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.6"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
